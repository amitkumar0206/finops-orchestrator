#!/bin/bash

# FinOps AI Cost Intelligence Platform - Deployment Script
# This script deploys the complete infrastructure and application stack
# Supports both fresh deployment and updates to existing infrastructure

set -euo pipefail

# Additional safety flags / traps
# Ensure ERR traps are inherited by functions/subshells (portable to macOS Bash 3.2)
set -E

# Global error trap to surface silent failures (e.g. docker push)
trap 'log_error "Unhandled error at line $LINENO: ${BASH_COMMAND}"; log_error "Aborting deployment."; exit 1' ERR

# Configuration
STACK_NAME="finops-intelligence-platform"
AWS_REGION="us-east-1"
ENVIRONMENT="production"
ENV_FILE="deployment.env"
FRESH_INSTALL=false
DEPLOYMENT_FAILED=false

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Functions
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Update deployment state in deployment.env file (preserves user configuration)
update_deployment_state() {
    local key=$1
    local value=$2
    
    # If deployment.env doesn't exist or doesn't have the DEPLOYMENT STATE marker, recreate structure
    if [ ! -f "$ENV_FILE" ] || ! grep -q "# DEPLOYMENT STATE" "$ENV_FILE"; then
        log_warning "deployment.env file missing or malformed, will recreate"
        return
    fi
    
    # Remove existing key if present in deployment state section
    sed -i.bak "/^# DEPLOYMENT STATE/,\$s/^${key}=.*$//" "$ENV_FILE" && rm -f "${ENV_FILE}.bak"
    
    # Append the new value
    echo "${key}=${value}" >> "$ENV_FILE"
}

# Save all deployment state to deployment.env file
save_deployment_state() {
    log_info "Saving deployment state to $ENV_FILE..."
    
    # Backup existing file if it exists
    if [ -f "$ENV_FILE" ]; then
        cp "$ENV_FILE" "${ENV_FILE}.backup"
    fi
    
    # If file doesn't exist or doesn't have proper structure, create it with template
    if [ ! -f "$ENV_FILE" ] || ! grep -q "# DEPLOYMENT STATE" "$ENV_FILE"; then
        cat > "$ENV_FILE" << 'EOF'
# FinOps Intelligence Platform - Configuration
# This single file contains all deployment configuration and state

# =============================================================================
# USER CONFIGURATION - Edit these values before deployment
# =============================================================================

# Custom Domain Configuration
USE_CUSTOM_DOMAIN=true
DOMAIN_NAME=
HOSTED_ZONE_ID=
CREATE_ACM_CERTIFICATE=true

# AWS Configuration  
AWS_REGION=us-east-1
ENVIRONMENT=production

# Bedrock AI Model (use inference profile for Nova models)
BEDROCK_MODEL=us.amazon.nova-premier-v1:0

# Universal Parameter Schema (UPS) Configuration
# Toggle to disable LLM extraction (use heuristic-only mode for deterministic testing)
# UPS_DISABLE_LLM=true
# Path to intent confidence threshold overrides (optional, auto-calibrated if missing)
# INTENT_THRESHOLDS_PATH=/app/intent_thresholds.json

# CUR (Cost and Usage Report) Configuration
AWS_CUR_DATABASE=cost_usage_db
AWS_CUR_TABLE=cur_data
ATHENA_WORKGROUP=finops-workgroup
# Optional: set a dedicated Athena results bucket. If left empty,
# the deploy script will default to
#   finops-intelligence-platform-athena-results-<AWS_ACCOUNT_ID>
# Example:
# ATHENA_RESULTS_BUCKET=finops-intelligence-platform-athena-results-123456789012

# =============================================================================
# DEPLOYMENT STATE - Auto-generated by deploy script, do not edit manually
# =============================================================================

EOF
    fi
    
    # Update deployment state values
    update_deployment_state "STACK_NAME" "$STACK_NAME"
    update_deployment_state "DB_PASSWORD" "$DB_PASSWORD"
    update_deployment_state "S3_BUCKET" "$S3_BUCKET"
    update_deployment_state "BEDROCK_MODEL" "$BEDROCK_MODEL"
    update_deployment_state "CUR_S3_BUCKET" "$CUR_S3_BUCKET"
    update_deployment_state "CUR_S3_PREFIX" "$CUR_S3_PREFIX"
    update_deployment_state "AWS_CUR_DATABASE" "$AWS_CUR_DATABASE"
    update_deployment_state "AWS_CUR_TABLE" "$AWS_CUR_TABLE"
    update_deployment_state "ATHENA_RESULTS_BUCKET" "$ATHENA_RESULTS_BUCKET"
    update_deployment_state "ATHENA_OUTPUT_LOCATION" "$ATHENA_OUTPUT_LOCATION"
    update_deployment_state "ATHENA_WORKGROUP" "$ATHENA_WORKGROUP"
    
    if [ -n "$DOMAIN_NAME" ]; then
        update_deployment_state "DOMAIN_NAME" "$DOMAIN_NAME"
        update_deployment_state "HOSTED_ZONE_ID" "$HOSTED_ZONE_ID"
        update_deployment_state "CREATE_ACM_CERTIFICATE" "$CREATE_CERT_FLAG"
    fi
    
    log_success "Deployment state saved to $ENV_FILE"
}

# Safe read that won't abort the script under `set -e`
safe_read() {
    # Usage: safe_read "Prompt" VAR_NAME DEFAULT
    local __prompt="$1"; shift
    local __varname="$1"; shift
    local __default="$1"
    local __input=""
    # Use -r to avoid backslash escapes; if read fails (EOF), keep empty input
    if read -r -p "$__prompt" __input; then
        :
    else
        __input=""
    fi
    if [ -z "$__input" ]; then
        eval "$__varname=\"$__default\""
    else
        eval "$__varname=\"$__input\""
    fi
    return 0
}

# Validate AWS credentials and provide helpful error messages
validate_aws_credentials() {
    local error_output
    error_output=$(aws sts get-caller-identity 2>&1) || {
        echo ""
        echo "======================================================================"
        echo "                    ❌ AWS AUTHENTICATION FAILED"
        echo "======================================================================"
        echo ""
        
        if echo "$error_output" | grep -qi "ExpiredToken"; then
            log_error "Your AWS security token has expired."
            echo ""
            echo "To fix this issue:"
            echo "  1. Refresh your AWS credentials"
            echo "  2. If using AWS SSO, run: aws sso login --profile <your-profile>"
            echo "  3. If using temporary credentials, obtain new ones from your AWS administrator"
            echo "  4. If using access keys, verify they are still valid in the AWS Console"
        elif echo "$error_output" | grep -qi "InvalidClientTokenId"; then
            log_error "Your AWS access key ID is invalid or does not exist."
            echo ""
            echo "To fix this issue:"
            echo "  1. Verify your AWS credentials are correctly configured"
            echo "  2. Run: aws configure"
            echo "  3. Or set environment variables: AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY"
        elif echo "$error_output" | grep -qi "SignatureDoesNotMatch"; then
            log_error "Your AWS secret access key is incorrect."
            echo ""
            echo "To fix this issue:"
            echo "  1. Verify your AWS credentials"
            echo "  2. Run: aws configure"
            echo "  3. Ensure your secret access key matches your access key ID"
        elif echo "$error_output" | grep -qi "Could not connect\|Network\|Connection"; then
            log_error "Unable to connect to AWS. Please check your internet connection or corporate proxy."
            echo ""
            echo "If you're on a corporate network, ensure your proxy and root CA are configured for the AWS CLI:"
            echo "  export HTTPS_PROXY=http://<proxy-host>:<port>"
            echo "  export NO_PROXY=169.254.169.254,localhost,127.0.0.1"
            echo "  export AWS_CA_BUNDLE=/path/to/corporate-root-ca.pem  # if SSL inspection is enabled"
        else
            log_error "AWS credentials validation failed."
            echo ""
            echo "Error details:"
            echo "$error_output"
            echo ""
            echo "To fix this issue:"
            echo "  1. Run: aws configure"
            echo "  2. Verify your credentials are valid in the AWS Console"
            echo "  3. Check that your IAM user/role has necessary permissions"
        fi
        
        echo ""
        echo "======================================================================"
        echo ""
        return 1
    }
    return 0
}

# Simple retry helper with exponential backoff (5s, 10s, 20s)
run_with_retry() {
    local attempts="${1:-3}"; shift
    local n=1
    local delay=5
    while true; do
        if "$@"; then
            return 0
        fi
        if [[ $n -lt $attempts ]]; then
            log_warning "Command failed (attempt $n/$attempts). Retrying in ${delay}s..."
            sleep "$delay"
            n=$((n+1))
            delay=$((delay*2))
        else
            log_error "Command failed after $attempts attempts."
            return 1
        fi
    done
}

# Rollback function for deployment failures
rollback_deployment() {
    local reason="$1"
    log_error "Deployment failed: $reason"
    log_warning "Initiating rollback procedure..."
    
    DEPLOYMENT_FAILED=true
    
    # Check if services stack was created/updated
    if aws cloudformation describe-stacks --stack-name "${STACK_NAME}-services" --region "$AWS_REGION" &>/dev/null; then
        local services_status=$(aws cloudformation describe-stacks \
            --stack-name "${STACK_NAME}-services" \
            --region "$AWS_REGION" \
            --query 'Stacks[0].StackStatus' \
            --output text 2>/dev/null || echo "UNKNOWN")
        
        if [[ "$services_status" == *"ROLLBACK"* ]] || [[ "$services_status" == *"FAILED"* ]]; then
            log_warning "Services stack in failed state: $services_status"
            log_info "You may want to run: ./deploy.sh destroy"
        fi
    fi
    
    # Check if main stack was created/updated
    if aws cloudformation describe-stacks --stack-name "$STACK_NAME" --region "$AWS_REGION" &>/dev/null; then
        local main_status=$(aws cloudformation describe-stacks \
            --stack-name "$STACK_NAME" \
            --region "$AWS_REGION" \
            --query 'Stacks[0].StackStatus' \
            --output text 2>/dev/null || echo "UNKNOWN")
        
        if [[ "$main_status" == *"ROLLBACK"* ]] || [[ "$main_status" == *"FAILED"* ]]; then
            log_warning "Main stack in failed state: $main_status"
            log_warning "CloudFormation will automatically rollback failed stacks."
            log_info "Monitor rollback: aws cloudformation describe-stacks --stack-name $STACK_NAME --region $AWS_REGION"
        fi
    fi
    
    log_warning "Rollback complete. Review the errors above and retry deployment."
    exit 1
}

# Comprehensive pre-flight validation
check_prerequisites() {
    log_info "Running pre-flight validation..."
    echo ""
    
    local validation_failed=false
    local warnings_found=false
    
    # 1. Check required tools
    log_info "[1/7] Validating required tools..."
    
    if ! command -v aws &> /dev/null; then
        log_error "AWS CLI not found. Install: https://aws.amazon.com/cli/"
        validation_failed=true
    else
        local aws_version=$(aws --version 2>&1 | cut -d' ' -f1 | cut -d'/' -f2)
        log_success "AWS CLI installed (version $aws_version)"
    fi
    
    if ! command -v docker &> /dev/null; then
        log_error "Docker not found. Install: https://www.docker.com/get-started"
        validation_failed=true
    else
        if ! docker ps &> /dev/null; then
            log_error "Docker daemon not running. Start Docker Desktop."
            validation_failed=true
        else
            local docker_version=$(docker --version | cut -d' ' -f3 | tr -d ',')
            log_success "Docker running (version $docker_version)"
        fi
    fi
    
    if ! command -v jq &> /dev/null; then
        log_warning "jq not found (optional). Install for enhanced JSON processing: brew install jq"
        warnings_found=true
    fi
    
    # 2. Check AWS credentials
    log_info "[2/7] Validating AWS credentials..."
    if ! validate_aws_credentials; then
        validation_failed=true
    else
        local account_id=$(aws sts get-caller-identity --query Account --output text)
        local user_arn=$(aws sts get-caller-identity --query Arn --output text)
        log_success "AWS credentials valid (Account: $account_id)"
        log_info "Authenticated as: $user_arn"
    fi
    
    # 3. Check AWS region configuration
    log_info "[3/7] Validating AWS region..."
    if [ -z "$AWS_REGION" ]; then
        AWS_REGION=$(aws configure get region || echo "us-east-1")
        log_warning "AWS_REGION not set, using: $AWS_REGION"
        warnings_found=true
    else
        log_success "Target region: $AWS_REGION"
    fi
    
    # 4. Check required IAM permissions
    log_info "[4/7] Validating IAM permissions..."
    local permission_errors=0
    
    # Check CloudFormation permissions
    if ! aws cloudformation list-stacks --region "$AWS_REGION" --max-results 1 &>/dev/null; then
        log_error "Missing CloudFormation permissions"
        permission_errors=$((permission_errors + 1))
    fi
    
    # Check ECR permissions
    if ! aws ecr describe-repositories --region "$AWS_REGION" --max-results 1 &>/dev/null; then
        log_warning "Missing ECR permissions (required for Docker image push)"
        warnings_found=true
    fi
    
    # Check Bedrock permissions
    if ! aws bedrock list-foundation-models --region "us-east-1" --max-results 1 &>/dev/null; then
        log_warning "Missing Bedrock permissions (verify model access enabled)"
        warnings_found=true
    fi
    
    if [ $permission_errors -gt 0 ]; then
        log_error "Critical IAM permissions missing. Required: CloudFormation, ECS, RDS, S3, IAM"
        validation_failed=true
    else
        log_success "Core IAM permissions validated"
    fi
    
    # 5. Check Bedrock model access
    log_info "[5/7] Validating Bedrock model access..."
    BEDROCK_MODEL="${BEDROCK_MODEL:-us.amazon.nova-premier-v1:0}"
    
    # Test Bedrock access with the configured model
    if aws bedrock-runtime invoke-model \
        --model-id "$BEDROCK_MODEL" \
        --body '{"messages":[{"role":"user","content":[{"text":"Test"}]}],"inferenceConfig":{"max_new_tokens":10}}' \
        --cli-binary-format raw-in-base64-out \
        --region us-east-1 \
        /tmp/bedrock-test.json &>/dev/null; then
        log_success "Bedrock model access validated: $BEDROCK_MODEL"
        rm -f /tmp/bedrock-test.json
    else
        log_warning "Bedrock model '$BEDROCK_MODEL' access failed. Enable in AWS Console:"
        log_warning "https://console.aws.amazon.com/bedrock/home?region=us-east-1#/modelaccess"
        warnings_found=true
    fi
    
    # 6. Check disk space for Docker builds
    log_info "[6/7] Validating disk space..."
    if command -v df &> /dev/null; then
        local available_gb=$(df -h . | awk 'NR==2 {print $4}' | sed 's/Gi\?//')
        if [ -n "$available_gb" ]; then
            log_success "Available disk space: ${available_gb}GB"
            if [ "${available_gb%%.*}" -lt 10 ]; then
                log_warning "Low disk space (< 10GB). Docker builds may fail."
                warnings_found=true
            fi
        fi
    fi
    
    # 7. Validate deployment configuration
    log_info "[7/7] Validating deployment configuration..."
    
    if [ -f "$ENV_FILE" ]; then
        log_success "Found deployment.env configuration"
        
        # Check for critical configuration
        if grep -q "^AWS_CUR_DATABASE=" "$ENV_FILE" && grep -q "^AWS_CUR_TABLE=" "$ENV_FILE"; then
            log_success "Athena configuration present"
        else
            log_info "Athena configuration will be set during deployment"
        fi
    else
        log_info "No deployment.env found (will be created during deployment)"
    fi
    
    echo ""
    
    # Summary
    if [ "$validation_failed" = true ]; then
        log_error "Pre-flight validation failed. Please address the errors above."
        exit 1
    elif [ "$warnings_found" = true ]; then
        log_warning "Pre-flight validation completed with warnings."
        echo ""
        read -p "Continue deployment despite warnings? (yes/no): " continue_choice
        if [ "$continue_choice" != "yes" ]; then
            log_info "Deployment cancelled."
            exit 0
        fi
    else
        log_success "Pre-flight validation passed. Ready for deployment."
    fi
    
    echo ""
}

# Detect existing infrastructure
detect_existing_infrastructure() {
    log_info "Detecting existing infrastructure..."
    echo ""
    
    local stack_exists=false
    local cur_exists=false
    local glue_exists=false
    local athena_exists=false
    
    # Check CloudFormation stack
    if aws cloudformation describe-stacks --stack-name "$STACK_NAME" --region "$AWS_REGION" &>/dev/null; then
        log_success "Found existing CloudFormation stack: $STACK_NAME"
        stack_exists=true
    else
        log_info "No existing CloudFormation stack found"
    fi
    
    # Get AWS Account ID
    AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
    
    # Check Glue database (used by Athena for CUR data)
    if aws glue get-database --name cost_usage_db --region "$AWS_REGION" &>/dev/null; then
        log_success "Found existing Glue database: cost_usage_db"
        glue_exists=true
    else
        log_info "Glue database will be created during deployment"
    fi
    
    # Check Athena workgroup
    if aws athena get-work-group --work-group finops-workgroup --region "$AWS_REGION" &>/dev/null; then
        log_success "Found existing Athena workgroup: finops-workgroup"
        athena_exists=true
    else
        log_info "Athena workgroup will be created during deployment"
    fi
    
    echo ""
    
    # Determine deployment mode
    if [ "$stack_exists" = true ]; then
        log_warning "Existing infrastructure detected!"
        echo ""
        echo "Options:"
        echo "  1) Update existing infrastructure (recommended for changes)"
        echo "  2) Fresh install (destroys everything and rebuilds - DATA LOSS)"
        echo "  3) Cancel"
        echo ""
        read -p "Choose option (1-3): " deploy_choice
        
        case $deploy_choice in
            1)
                FRESH_INSTALL=false
                log_info "Proceeding with infrastructure update..."
                ;;
            2)
                log_error "⚠️  WARNING: Fresh install will DELETE all existing infrastructure and data!"
                read -p "Type 'DESTROY' to confirm: " confirm
                if [ "$confirm" = "DESTROY" ]; then
                    FRESH_INSTALL=true
                    destroy_infrastructure
                    log_info "Proceeding with fresh installation..."
                else
                    log_info "Fresh install cancelled."
                    exit 0
                fi
                ;;
            3)
                log_info "Deployment cancelled."
                exit 0
                ;;
            *)
                log_error "Invalid option. Exiting."
                exit 1
                ;;
        esac
    else
        log_info "No existing infrastructure found. Proceeding with fresh installation..."
        FRESH_INSTALL=true
    fi
}

# Setup one-time infrastructure components
setup_one_time_components() {
    local cur_exists=$1
    local glue_exists=$2
    local athena_exists=$3
    
    log_info "Setting up one-time infrastructure components..."
    
    # Get AWS Account ID (needed for bucket name)
    AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
    
    # Initialize S3_BUCKET to empty string (required for set -u)
    S3_BUCKET=""
    ATHENA_RESULTS_BUCKET=""
    
    # Get S3 bucket from state or generate new one
    if [ -f "$ENV_FILE" ]; then
        S3_BUCKET=$(awk -F'=' '$1=="S3_BUCKET"{print $2}' "$ENV_FILE")
    fi
    
    if [ -z "$S3_BUCKET" ] && [ -f "$ENV_FILE" ]; then
        S3_BUCKET=$(awk -F'=' '$1=="S3_BUCKET"{print $2}' "$ENV_FILE")
    fi
    
    if [ -z "$S3_BUCKET" ]; then
        # Ensure globally unique bucket name by appending AWS account ID
        S3_BUCKET="finops-intelligence-platform-data-${AWS_ACCOUNT_ID}"
    fi

    # Determine results bucket (prefer value from deployment.env if present)
    if [ -f "$ENV_FILE" ]; then
        ATHENA_RESULTS_BUCKET=$(awk -F'=' '$1=="ATHENA_RESULTS_BUCKET"{print $2}' "$ENV_FILE")
    fi
    if [ -z "$ATHENA_RESULTS_BUCKET" ]; then
        ATHENA_RESULTS_BUCKET="finops-intelligence-platform-athena-results-${AWS_ACCOUNT_ID}"
    fi
    
    # Setup CUR/Data Export
    if [ "$cur_exists" = false ]; then
        # Allow teams to run in legacy CUR-only mode without creating the Standard Data Export
        if [ "${USE_LEGACY_CUR_ONLY}" = "true" ]; then
            log_info "Using Legacy CUR mode (USE_LEGACY_CUR_ONLY=true)"
            log_info "Standard Data Export setup skipped - expecting legacy CUR configuration"
            log_info "To enable Standard Data Export: export USE_LEGACY_CUR_ONLY=false"
        else
            log_info "Using Standard Data Export mode (USE_LEGACY_CUR_ONLY=false)"
            log_info "Setting up AWS Cost and Usage Data Export..."
            if [ -f "scripts/setup/setup-cur.sh" ]; then
                chmod +x scripts/setup/setup-cur.sh
                # Pass bucket name and flag to setup-cur.sh
                export CUR_BUCKET_NAME="$S3_BUCKET"
                export STACK_NAME="$STACK_NAME"
                export AWS_REGION="$AWS_REGION"
                export RUNNING_FROM_DEPLOY="true"
                export SKIP_BUCKET_CREATION="true"  # CloudFormation will create the bucket
                scripts/setup/setup-cur.sh || log_warning "Data export setup failed or was skipped"
                unset RUNNING_FROM_DEPLOY
                unset SKIP_BUCKET_CREATION
            else
                log_warning "setup-cur.sh not found. Please run it manually later."
            fi
        fi
    fi
    
    # Setup Glue Database (might already be created by setup-cur.sh)
    if [ "$glue_exists" = false ]; then
        log_info "Ensuring Glue database exists..."
        aws glue create-database \
            --database-input "{\"Name\":\"cost_usage_db\",\"Description\":\"Database for AWS Cost and Usage Reports\"}" \
            --region "$AWS_REGION" 2>/dev/null || log_info "Glue database already exists"
    fi
    
    # Ensure Athena Results bucket exists/configured
    if ! aws s3api head-bucket --bucket "$ATHENA_RESULTS_BUCKET" --region "$AWS_REGION" >/dev/null 2>&1; then
        log_info "Creating Athena results bucket: $ATHENA_RESULTS_BUCKET"
        if aws s3 mb "s3://$ATHENA_RESULTS_BUCKET" --region "$AWS_REGION" >/dev/null 2>&1; then
            aws s3api put-bucket-encryption \
                --bucket "$ATHENA_RESULTS_BUCKET" \
                --server-side-encryption-configuration '{"Rules":[{"ApplyServerSideEncryptionByDefault":{"SSEAlgorithm":"AES256"},"BucketKeyEnabled":true}]}' \
                --region "$AWS_REGION" >/dev/null 2>&1 || true
            aws s3api put-bucket-versioning \
                --bucket "$ATHENA_RESULTS_BUCKET" \
                --versioning-configuration Status=Enabled \
                --region "$AWS_REGION" >/dev/null 2>&1 || true
            aws s3api put-public-access-block \
                --bucket "$ATHENA_RESULTS_BUCKET" \
                --public-access-block-configuration "BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true" \
                --region "$AWS_REGION" >/dev/null 2>&1 || true
            log_success "Athena results bucket created"
        else
            log_warning "Could not create results bucket; ensure it exists and is accessible"
        fi
    else
        log_info "Using existing Athena results bucket: $ATHENA_RESULTS_BUCKET"
    fi

    # Setup Athena Workgroup (might already be created by setup-cur.sh)
    if [ "$athena_exists" = false ]; then
        log_info "Ensuring Athena workgroup exists..."
        aws athena create-work-group \
            --name finops-workgroup \
            --configuration "ResultConfiguration={OutputLocation=s3://$ATHENA_RESULTS_BUCKET/}" \
            --region "$AWS_REGION" 2>/dev/null || log_info "Athena workgroup already exists"
    fi
    
    log_success "One-time components setup completed."
}

# Deploy CloudFormation stack
deploy_infrastructure() {
    if [ "$FRESH_INSTALL" = true ]; then
        log_info "Deploying fresh infrastructure stack..."
    else
        log_info "Updating existing infrastructure stack..."
    fi

    # Get AWS Account ID (needed for bucket name)
    AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)

    # Load previously used bucket if available
    PREVIOUS_S3_BUCKET=""
    if [ -f "$ENV_FILE" ]; then
        PREVIOUS_S3_BUCKET=$(awk -F'=' '$1=="S3_BUCKET"{print $2}' "$ENV_FILE")
        if [ -n "$PREVIOUS_S3_BUCKET" ]; then
            log_info "Found existing S3 bucket in $ENV_FILE: $PREVIOUS_S3_BUCKET"
        fi
    fi
    
    # Generate random password for database (RDS doesn't allow /, @, ", or spaces)
    # Use alphanumeric + allowed special characters: !#$%&*+-<=>?^_`{|}~
    if [ -f "$ENV_FILE" ] && grep -q "DB_PASSWORD" deployment.env; then
        DB_PASSWORD=$(grep "DB_PASSWORD" deployment.env | cut -d'=' -f2-)
        log_info "Reusing existing database password from deployment.env"
    else
        DB_PASSWORD=$(openssl rand -base64 32 | tr -dc 'A-Za-z0-9!#$%&*+<=>?^_~' | head -c 32)
        log_info "Generated new database password"
    fi
    
    # Get S3 bucket name (must be globally unique)
    if [ -n "$PREVIOUS_S3_BUCKET" ]; then
        S3_BUCKET="$PREVIOUS_S3_BUCKET"
        log_info "Reusing previous S3 bucket: $S3_BUCKET"
    else
        # Ensure globally unique bucket name by appending AWS account ID
        S3_BUCKET="finops-intelligence-platform-data-${AWS_ACCOUNT_ID}"
        log_info "Using S3 bucket name: $S3_BUCKET"
    fi
    
    # Check if S3 bucket already exists
    BUCKET_ALREADY_EXISTS=false
    if aws s3api head-bucket --bucket "$S3_BUCKET" --region "$AWS_REGION" 2>/dev/null; then
        log_warning "S3 bucket '$S3_BUCKET' already exists - CloudFormation will import/use existing bucket"
        BUCKET_ALREADY_EXISTS=true
    elif [ -n "$PREVIOUS_S3_BUCKET" ] && [ "$S3_BUCKET" = "$PREVIOUS_S3_BUCKET" ]; then
        # If we're reusing a previous bucket name from deployment.env, assume it exists
        # even if head-bucket failed (could be transient API issue)
        log_warning "Using S3 bucket from previous deployment: $S3_BUCKET (assuming it exists)"
        BUCKET_ALREADY_EXISTS=true
    fi
    
    # Prompt for Bedrock model selection
    echo ""
    echo "Available Bedrock models:"
    echo "1) Amazon Nova Pro 2.0 (Default - Best for production FinOps analysis)"
    echo "2) Amazon Nova Lite 2.0 (Balanced performance and cost)"
    echo "3) Amazon Nova Micro 2.0 (Fastest, most cost-effective)"
    echo "4) Meta Llama 3 70B (Powerful open-source alternative)"
    echo "5) Mistral Large (European alternative)"
    echo ""
    read -p "Select Bedrock model (1-5, default: 1): " MODEL_CHOICE
    
    case $MODEL_CHOICE in
        2) BEDROCK_MODEL="amazon.nova-2-lite-v1:0" ;;
        3) BEDROCK_MODEL="amazon.nova-2-sonic-v1:0" ;;
        4) BEDROCK_MODEL="meta.llama3-70b-instruct-v1:0" ;;
        5) BEDROCK_MODEL="mistral.mistral-large-2402-v1:0" ;;
        *) BEDROCK_MODEL="us.amazon.nova-premier-v1:0" ;;
    esac
    
    log_info "Using Bedrock model: $BEDROCK_MODEL"
    
    # Load custom domain configuration from config file
    echo ""
    log_info "Loading custom domain configuration from $ENV_FILE..."
    
    # Set defaults
    USE_CUSTOM_DOMAIN=false
    DOMAIN_NAME=""
    HOSTED_ZONE_ID=""
    CREATE_CERT_FLAG="false"
    
    # Load configuration from file if it exists
    if [ -f "$ENV_FILE" ]; then
        # Source the config file (ignoring comments and empty lines)
        while IFS='=' read -r key value; do
            # Skip comments and empty lines
            [[ "$key" =~ ^#.*$ ]] && continue
            [[ -z "$key" ]] && continue
            
            # Trim whitespace
            key=$(echo "$key" | xargs)
            value=$(echo "$value" | xargs)
            
            case "$key" in
                USE_CUSTOM_DOMAIN)
                    USE_CUSTOM_DOMAIN=$(echo "$value" | tr '[:upper:]' '[:lower:]')
                    ;;
                DOMAIN_NAME)
                    DOMAIN_NAME="$value"
                    ;;
                HOSTED_ZONE_ID)
                    HOSTED_ZONE_ID="$value"
                    ;;
                CREATE_ACM_CERTIFICATE)
                    CREATE_ACM_CERTIFICATE=$(echo "$value" | tr '[:upper:]' '[:lower:]')
                    ;;
                CUR_S3_PREFIX)
                    CUR_S3_PREFIX="$value"
                    ;;
            esac
        done < "$ENV_FILE"
        
        log_success "Configuration loaded from $ENV_FILE"
    else
        log_warning "Config file not found: $ENV_FILE"
        log_info "Using default configuration (no custom domain)"
    fi
    
    # Process custom domain configuration
    if [[ "$USE_CUSTOM_DOMAIN" == "true" ]]; then
        if [ -z "$DOMAIN_NAME" ] || [ -z "$HOSTED_ZONE_ID" ]; then
            log_error "Custom domain enabled but DOMAIN_NAME or HOSTED_ZONE_ID not configured in $ENV_FILE"
            exit 1
        fi
        
        log_info "Custom domain enabled: $DOMAIN_NAME"
        log_info "Hosted Zone ID: $HOSTED_ZONE_ID"
        
        # Check and clean up existing DNS record
        log_info "Checking for existing DNS record..."
        EXISTING_RECORD=$(aws route53 list-resource-record-sets \
            --hosted-zone-id "$HOSTED_ZONE_ID" \
            --query "ResourceRecordSets[?Name=='${DOMAIN_NAME}.']" \
            --output json 2>/dev/null)
        
        if [ -n "$EXISTING_RECORD" ] && [ "$EXISTING_RECORD" != "[]" ]; then
            log_warning "Found existing DNS record for $DOMAIN_NAME - it will be updated automatically"
        fi
        
        # Set certificate creation flag (default to true when custom domain is enabled)
        if [[ "$CREATE_ACM_CERTIFICATE" == "false" ]]; then
            CREATE_CERT_FLAG="false"
            log_warning "HTTPS certificate creation disabled - using HTTP only"
        else
            CREATE_CERT_FLAG="true"
            log_info "HTTPS certificate will be created automatically"
        fi
        
        log_info "Custom domain: $DOMAIN_NAME (HTTPS: $CREATE_CERT_FLAG)"
    else
        DOMAIN_NAME=""
        HOSTED_ZONE_ID=""
        CREATE_CERT_FLAG="false"
        log_info "Custom domain disabled - using ALB DNS name"
    fi
    
    # Pre-create S3 bucket if it doesn't exist (CloudFormation no longer manages this bucket)
    # This ensures idempotent deployments without "AlreadyExists" errors
    if [ "$BUCKET_ALREADY_EXISTS" = false ]; then
        log_info "Creating S3 bucket: $S3_BUCKET"
        if aws s3 mb "s3://$S3_BUCKET" --region "$AWS_REGION" 2>/dev/null; then
            log_success "S3 bucket created successfully"
            
            # Apply encryption
            log_info "Configuring bucket encryption..."
            aws s3api put-bucket-encryption \
                --bucket "$S3_BUCKET" \
                --server-side-encryption-configuration '{"Rules":[{"ApplyServerSideEncryptionByDefault":{"SSEAlgorithm":"AES256"},"BucketKeyEnabled":true}]}' \
                --region "$AWS_REGION" 2>/dev/null || log_warning "Could not enable encryption"
            
            # Enable versioning
            log_info "Enabling bucket versioning..."
            aws s3api put-bucket-versioning \
                --bucket "$S3_BUCKET" \
                --versioning-configuration Status=Enabled \
                --region "$AWS_REGION" 2>/dev/null || log_warning "Could not enable versioning"
            
            # Block public access
            log_info "Configuring public access block..."
            aws s3api put-public-access-block \
                --bucket "$S3_BUCKET" \
                --public-access-block-configuration "BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true" \
                --region "$AWS_REGION" 2>/dev/null || log_warning "Could not configure public access block"
            
            # Add lifecycle rule for incomplete multipart uploads
            log_info "Configuring lifecycle rules..."
            cat > /tmp/lifecycle-config.json << 'LIFECYCLE_EOF'
{
    "Rules": [{
        "Id": "DeleteIncompleteMultipartUploads",
        "Status": "Enabled",
        "AbortIncompleteMultipartUpload": {
            "DaysAfterInitiation": 1
        },
        "Filter": {}
    }]
}
LIFECYCLE_EOF
            # Brief delay for eventual consistency
            sleep 2
            if aws s3api put-bucket-lifecycle-configuration \
                --bucket "$S3_BUCKET" \
                --lifecycle-configuration file:///tmp/lifecycle-config.json \
                --region "$AWS_REGION" 2>/dev/null; then
                log_success "Lifecycle rules configured"
            else
                log_warning "Could not configure lifecycle rules (bucket will still work fine)"
            fi
            rm -f /tmp/lifecycle-config.json
            
            log_success "S3 bucket configured successfully"
            BUCKET_ALREADY_EXISTS=true
        else
            log_error "Failed to create S3 bucket: $S3_BUCKET"
            log_error "This could mean:"
            log_error "  1. Bucket already exists (will attempt to use it)"
            log_error "  2. Bucket name is globally taken by another AWS account"
            log_error "  3. Insufficient permissions"
            log_info "Checking if bucket exists..."
            if aws s3api head-bucket --bucket "$S3_BUCKET" --region "$AWS_REGION" 2>/dev/null; then
                log_warning "Bucket exists - continuing with existing bucket"
                BUCKET_ALREADY_EXISTS=true
            else
                log_error "Cannot create or access bucket. Please check:"
                log_error "  - Bucket name is globally unique"
                log_error "  - You have s3:CreateBucket permissions"
                log_error "  - No conflicting bucket policies"
                exit 1
            fi
        fi
    else
        log_info "Using existing S3 bucket: $S3_BUCKET"
    fi
    
    # ============================================================================
    # CUR Configuration (Set defaults - CUR is configured manually after deployment)
    # ============================================================================
    echo ""
    log_info "=== CUR Configuration ==="
    
    # Load CUR configuration from deployment.env if exists
    if [ -f "$ENV_FILE" ]; then
        source "$ENV_FILE"
    fi
    
    # Set CUR configuration with defaults (used by CloudFormation and application)
    CUR_S3_BUCKET="${CUR_S3_BUCKET:-$S3_BUCKET}"
    CUR_S3_PREFIX="${CUR_S3_PREFIX:-cost-exports/finops-cost-export}"
    AWS_CUR_DATABASE="${AWS_CUR_DATABASE:-cost_usage_db}"
    AWS_CUR_TABLE="${AWS_CUR_TABLE:-cur_data}"
    ATHENA_RESULTS_BUCKET="${ATHENA_RESULTS_BUCKET:-finops-intelligence-platform-athena-results-${AWS_ACCOUNT_ID}}"
    ATHENA_OUTPUT_LOCATION="${ATHENA_OUTPUT_LOCATION:-s3://${ATHENA_RESULTS_BUCKET}/}"
    ATHENA_WORKGROUP="${ATHENA_WORKGROUP:-finops-workgroup}"
    
    log_info "CUR S3 Location: s3://$CUR_S3_BUCKET/$CUR_S3_PREFIX"
    log_info "Athena Database: $AWS_CUR_DATABASE"
    log_info "Athena Table: $AWS_CUR_TABLE"
    
    # Check if CUR data already exists (e.g., re-deployment or pre-configured)
    # This is optional - deployment proceeds regardless
    CUR_CONFIGURED=false
    log_info "Checking if CUR data already exists..."
    
    if aws s3 ls "s3://${CUR_S3_BUCKET}/${CUR_S3_PREFIX}/" --region "$AWS_REGION" 2>/dev/null | grep -q "\.parquet"; then
        log_success "✓ CUR data found - table will be immediately queryable"
        CUR_CONFIGURED=true
    else
        log_info "ℹ️  No CUR data yet - table will be created, data queryable once CUR delivers"
        CUR_CONFIGURED=false
    fi
    
    echo ""
    log_info "=== Creating Athena Infrastructure ==="
    
    # Create Athena workgroup
    if ! aws athena get-work-group --work-group "$ATHENA_WORKGROUP" --region "$AWS_REGION" &>/dev/null; then
        log_info "Creating Athena workgroup: $ATHENA_WORKGROUP"
        aws athena create-work-group \
            --name "$ATHENA_WORKGROUP" \
            --configuration "ResultConfiguration={OutputLocation=${ATHENA_OUTPUT_LOCATION}}" \
            --region "$AWS_REGION" 2>/dev/null || log_warning "Failed to create Athena workgroup"
    else
        log_success "✓ Athena workgroup exists: $ATHENA_WORKGROUP"
    fi
    
    # Create Athena table with partition projection (even if no data yet)
    if [ -f "scripts/setup/setup-athena-cur.sh" ]; then
        chmod +x scripts/setup/setup-athena-cur.sh
        log_info "Creating Athena CUR table with partition projection..."
        log_info "Note: Table creation works even without data - queries will return results once CUR delivers data"
        
        if bash scripts/setup/setup-athena-cur.sh \
            "$CUR_S3_BUCKET" \
            "$CUR_S3_PREFIX" \
            "$AWS_CUR_DATABASE" \
            "$AWS_CUR_TABLE" \
            "$AWS_REGION" \
            "$ATHENA_OUTPUT_LOCATION"; then
            log_success "Athena table created with partition projection"
            if [ "$CUR_CONFIGURED" = "true" ]; then
                log_success "CUR data exists - table is queryable"
            else
                log_info "Table ready - awaiting CUR data delivery"
            fi
        else
            log_warning "Athena table creation encountered issues (see logs above)"
            log_info "Manual creation available: scripts/setup/setup-athena-cur.sh"
        fi
    else
        log_error "setup-athena-cur.sh not found - skipping Athena table creation"
    fi
    
    echo ""
    log_info "=== Deploying CloudFormation Stack ==="
    
    # Check if Glue database already exists
    CREATE_DATABASE_FLAG="true"
    if aws glue get-database --name "$AWS_CUR_DATABASE" --region "$AWS_REGION" &>/dev/null; then
        log_info "Glue database '$AWS_CUR_DATABASE' already exists - CloudFormation will skip creation"
        CREATE_DATABASE_FLAG="false"
    else
        log_info "Glue database '$AWS_CUR_DATABASE' does not exist - CloudFormation will create it"
        CREATE_DATABASE_FLAG="true"
    fi
    
    # CloudFormation parameters (defined here after CUR config is set)
    CFN_PARAMS=(
        Environment="$ENVIRONMENT"
        DatabasePassword="$DB_PASSWORD"
        BedrockModelId="$BEDROCK_MODEL"
        S3BucketName="$S3_BUCKET"
        DomainName="$DOMAIN_NAME"
        HostedZoneId="$HOSTED_ZONE_ID"
        CreateACMCertificate="$CREATE_CERT_FLAG"
        CurReportBucketName="$CUR_S3_BUCKET"
        CurReportPrefix="$CUR_S3_PREFIX"
        CurDatabaseName="$AWS_CUR_DATABASE"
        CreateCurDatabase="$CREATE_DATABASE_FLAG"
    )
    
    log_info "[DEBUG] Main stack BedrockModelId: $BEDROCK_MODEL"
    
    run_with_retry 3 aws cloudformation deploy \
        --template-file infrastructure/cloudformation/main-stack.yaml \
        --stack-name "$STACK_NAME" \
        --parameter-overrides "${CFN_PARAMS[@]}" \
        --capabilities CAPABILITY_NAMED_IAM \
        --region "$AWS_REGION"
    
    if [ $? -eq 0 ]; then
        log_success "Infrastructure stack deployed successfully."
        
        # Save deployment state to deployment.env
        save_deployment_state
        
        # Store database password in SSM Parameter Store for future deployments
        log_info "Storing database password in SSM Parameter Store..."
        aws ssm put-parameter \
            --name "/${STACK_NAME}/database/password" \
            --value "$DB_PASSWORD" \
            --type "SecureString" \
            --overwrite \
            --region "$AWS_REGION" \
            --description "Database password for ${STACK_NAME} RDS instance" 2>/dev/null
        
        if [ $? -eq 0 ]; then
            log_success "Database password stored in SSM Parameter Store"
        else
            log_warning "Failed to store password in SSM (this is optional)"
        fi

    # Ensure CUR pipeline (Lambda + Athena table) so fresh installs are queryable immediately
    setup_cur_pipeline_if_needed
        
        # Check certificate status if HTTPS was requested
        if [ -n "$DOMAIN_NAME" ] && [[ "$CREATE_CERT_FLAG" == "true" ]]; then
            log_info "Checking ACM certificate status..."
            
            CERT_ARN=$(aws cloudformation describe-stacks \
                --stack-name "$STACK_NAME" \
                --region "$AWS_REGION" \
                --query 'Stacks[0].Outputs[?OutputKey==`CertificateArn`].OutputValue' \
                --output text 2>/dev/null || echo "")
            
            if [ -n "$CERT_ARN" ] && [ "$CERT_ARN" != "None" ]; then
                log_success "ACM certificate created: $CERT_ARN"
                log_info "Certificate validation in progress (DNS records auto-created)"
                log_info "HTTPS will be available once validation completes (5-10 minutes)"
                
                # Check validation status
                CERT_STATUS=$(aws acm describe-certificate --certificate-arn "$CERT_ARN" --region "$AWS_REGION" --query 'Certificate.Status' --output text 2>/dev/null || echo "UNKNOWN")
                if [ "$CERT_STATUS" = "ISSUED" ]; then
                    log_success "Certificate already validated and issued"
                elif [ "$CERT_STATUS" = "PENDING_VALIDATION" ]; then
                    log_info "Certificate status: Pending validation (automatic via DNS)"
                else
                    log_warning "Certificate status: $CERT_STATUS"
                fi
            else
                log_warning "ACM certificate creation failed or not configured"
                log_info "Application will use HTTP instead of HTTPS"
                log_info "Manual certificate creation available in ACM console"
            fi
        fi
        
        # Display application URL
        APP_URL=$(aws cloudformation describe-stacks \
            --stack-name "$STACK_NAME" \
            --region "$AWS_REGION" \
            --query 'Stacks[0].Outputs[?OutputKey==`ApplicationURL`].OutputValue' \
            --output text 2>/dev/null || echo "")
        
        if [ -z "$APP_URL" ] && [ -n "$DOMAIN_NAME" ]; then
            # Fallback to custom domain if ApplicationURL output not available
            if [ -n "$CERT_ARN" ] && [ "$CERT_ARN" != "None" ]; then
                APP_URL="https://$DOMAIN_NAME"
            else
                APP_URL="http://$DOMAIN_NAME"
            fi
        fi
        
        if [ -n "$APP_URL" ]; then
            log_success "Application URL: $APP_URL"
        fi
    else
        log_error "Infrastructure deployment failed."
        rollback_deployment "CloudFormation stack deployment failed"
    fi
}

# Validate Athena data configuration after setup
validate_athena_data() {
    echo ""
    log_info "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    log_info "  Athena Data Configuration Validation"
    log_info "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    echo ""
    
    # Run post-setup validation if available
    if [ -f "scripts/utilities/validate-post-setup.sh" ]; then
        log_info "Running Athena data validation..."
        chmod +x scripts/utilities/validate-post-setup.sh
        
        if bash scripts/utilities/validate-post-setup.sh; then
            log_success "Athena data validation passed"
        else
            log_warning "Athena data validation detected issues"
            log_info "This is normal if CUR data hasn't been generated yet (takes 24 hours)"
            log_info "You can re-run validation later: ./scripts/utilities/validate-post-setup.sh"
        fi
    else
        log_info "Athena validation script not found, skipping"
        log_info "You can verify data manually: ./scripts/utilities/verify-athena-data.sh"
    fi
    echo ""
}

# Ensure CUR 2.0 pipeline is wired: archive manifests, deploy Lambda archiver, and (re)create Athena cur_data table
setup_cur_pipeline_if_needed() {
    echo ""
    log_info "=== Ensuring CUR data pipeline (Lambda + Athena view) ==="
    local db_name="${AWS_CUR_DATABASE:-cost_usage_db}"
    local table_name="${AWS_CUR_TABLE:-cur_data}"

    # Quick check: does the table/view already exist?
    local qid state
    qid=$(aws athena start-query-execution \
        --query-string "SHOW TABLES IN ${db_name}" \
        --work-group "${ATHENA_WORKGROUP:-finops-workgroup}" \
        --result-configuration OutputLocation="${ATHENA_OUTPUT_LOCATION:-s3://${ATHENA_RESULTS_BUCKET}/}" \
        --region "$AWS_REGION" --query 'QueryExecutionId' --output text 2>/dev/null || echo "")
    if [ -n "$qid" ]; then
        sleep 3
        if aws athena get-query-results --query-execution-id "$qid" --region "$AWS_REGION" 2>/dev/null | grep -q "${table_name}"; then
            log_success "Athena view/table ${db_name}.${table_name} exists"
            
            # Check if Glue Crawler needs to be run (if CUR data exists but no timestamped tables)
            local glue_tables_count
            glue_tables_count=$(aws glue get-tables --database-name "$db_name" --region "$AWS_REGION" --query 'TableList | length(@)' --output text 2>/dev/null || echo "0")
            
            if [ "$glue_tables_count" -le 2 ]; then
                log_info "Few Glue tables found ($glue_tables_count), checking if Glue Crawler should be run..."
                
                # Check if CUR data exists in S3
                local cur_s3_path="s3://${S3_BUCKET}/${CUR_S3_PREFIX:-cost-exports/finops-cost-export}/"
                if aws s3 ls "$cur_s3_path" --region "$AWS_REGION" 2>/dev/null | grep -q "/"; then
                    log_info "CUR data found in S3, running Glue Crawler to discover tables..."
                    
                    # Create/run Glue Crawler
                    if [ -f "scripts/deployment/create_cur_with_crawler.sh" ]; then
                        chmod +x scripts/deployment/create_cur_with_crawler.sh
                        scripts/deployment/create_cur_with_crawler.sh || log_warning "Glue Crawler setup had issues"
                    fi
                    
                    # Recreate the unified view after Crawler discovers new tables
                    log_info "Recreating unified CUR view with latest tables..."
                    if [ -f "scripts/deployment/setup_cur_view.sh" ]; then
                        chmod +x scripts/deployment/setup_cur_view.sh
                        scripts/deployment/setup_cur_view.sh "$ENV_FILE" || log_warning "CUR view recreation had issues"
                    fi
                fi
            fi
            
            return 0
        fi
    fi

    # Run the setup script (idempotent): moves manifests, deploys Lambda, creates Athena table from a manifest schema
    if [ -f "scripts/setup/setup_cur_pipeline.sh" ]; then
        chmod +x scripts/setup/setup_cur_pipeline.sh
        if RUNNING_FROM_DEPLOY=1 scripts/setup/setup_cur_pipeline.sh; then
            log_success "CUR pipeline ensured."
        else
            log_warning "CUR pipeline setup encountered issues; will try unified view approach..."
        fi
    else
        log_info "Legacy CUR pipeline script not found, using unified view approach..."
    fi
    
    # After pipeline setup or if it failed, ensure unified view exists
    log_info "Setting up unified CUR view for querying timestamped manifest versions..."
    if [ -f "scripts/deployment/setup_cur_view.sh" ]; then
        chmod +x scripts/deployment/setup_cur_view.sh
        if scripts/deployment/setup_cur_view.sh "$ENV_FILE"; then
            log_success "✅ CUR unified view setup complete"
        else
            log_warning "CUR view setup had issues; backend will fall back to Cost Explorer API"
        fi
    else
        log_warning "scripts/deployment/setup_cur_view.sh not found; skipping CUR view setup."
        log_warning "Backend will rely on Cost Explorer API for cost data."
    fi
}

# Build and push Docker images
build_and_push_images() {
    log_info "Building and pushing Docker images..."
    
    # Get AWS account ID
    AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
    ECR_REGISTRY="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com"
    
    # Check ECR permissions
    log_info "Checking ECR permissions..."
    if ! aws ecr describe-repositories --region "$AWS_REGION" >/dev/null 2>&1; then
        log_warning "ECR access denied. Your AWS role lacks ECR permissions."
        log_warning "Skipping Docker image build and push."
        log_warning "Setting placeholder image URIs for deployment."
        
        # Set placeholder image URIs for deployment
        BACKEND_IMAGE_URI="${ECR_REGISTRY}/finops-backend:latest"
        FRONTEND_IMAGE_URI="${ECR_REGISTRY}/finops-frontend:latest"
        
        # Save to deployment.env
        echo "BACKEND_IMAGE_URI=$BACKEND_IMAGE_URI" >> "$ENV_FILE"
        echo "FRONTEND_IMAGE_URI=$FRONTEND_IMAGE_URI" >> "$ENV_FILE"
        
        log_warning "⚠️  IMPORTANT: You must manually build and push Docker images with ECR permissions."
        log_warning "   Run: ./deploy.sh update (with proper AWS credentials)"
        log_warning "   Or manually build and push images to ECR."
        return 0
    fi
    
    # Create ECR repositories
    aws ecr create-repository --repository-name finops-backend --region "$AWS_REGION" 2>/dev/null || true
    aws ecr create-repository --repository-name finops-frontend --region "$AWS_REGION" 2>/dev/null || true
    
    # Login to ECR
    log_info "Logging in to ECR..."
    ECR_PASSWORD=$(aws ecr get-login-password --region "$AWS_REGION")
    echo "$ECR_PASSWORD" | docker login --username AWS --password-stdin "$ECR_REGISTRY"
    log_success "ECR login successful"
    
    # Build and push backend image
    log_info "Building backend image (5-10 minutes)..."
    log_info "Installing dependencies with CPU-only PyTorch (optimized for size)..."
    echo ""
    log_info "TIP: For faster rebuilds, use: ./rebuild-backend.sh"
    log_info "     (Uses Docker cache, rebuilds backend only)"
    echo ""

    # Allow forcing a no-cache build to ensure latest code is included
    NO_CACHE_FLAG=""
    if [ "${NO_CACHE:-}" = "1" ]; then
        log_warning "Forcing Docker build without cache (NO_CACHE=1)"
        log_warning "This will download all dependencies again (~10-15 minutes extra)"
        log_warning "To use cache: unset NO_CACHE or run without NO_CACHE=1"
        NO_CACHE_FLAG="--no-cache"
    fi
    
    if ! DOCKER_BUILDKIT=1 docker build \
        --platform linux/amd64 \
        --build-arg BUILDKIT_INLINE_CACHE=1 \
        --progress=plain \
        ${NO_CACHE_FLAG} \
        -f backend/Dockerfile \
        -t finops-backend .; then
        log_error "Backend build failed!"
        rollback_deployment "Backend Docker image build failed"
    fi
    
    log_info "Tagging and pushing backend image..."
    if ! docker tag finops-backend:latest "${ECR_REGISTRY}/finops-backend:latest"; then
        log_error "Failed to tag backend image"
        rollback_deployment "Docker tag backend failed"
    fi
    if ! docker push "${ECR_REGISTRY}/finops-backend:latest"; then
        log_error "Failed to push backend image to ECR"
        rollback_deployment "Backend image push failed"
    fi
    log_success "Backend image pushed successfully"
    
    # Build and push frontend image
    log_info "Building frontend image (lightweight, ~2 minutes)..."
    if ! DOCKER_BUILDKIT=1 docker build \
        --platform linux/amd64 \
        --build-arg BUILDKIT_INLINE_CACHE=1 \
        --progress=plain \
        ${NO_CACHE_FLAG} \
        -f frontend/Dockerfile \
        -t finops-frontend .; then
        log_error "Frontend build failed!"
        rollback_deployment "Frontend Docker image build failed"
    fi
    
    log_info "Tagging and pushing frontend image..."
    if ! docker tag finops-frontend:latest "${ECR_REGISTRY}/finops-frontend:latest"; then
        log_error "Failed to tag frontend image"
        rollback_deployment "Docker tag frontend failed"
    fi
    if ! docker push "${ECR_REGISTRY}/finops-frontend:latest"; then
        log_error "Failed to push frontend image to ECR"
        rollback_deployment "Frontend image push failed"
    fi
    log_success "Frontend image pushed successfully"
    
    log_success "Docker images built and pushed successfully."
    
    # Save image URIs
    echo "BACKEND_IMAGE_URI=${ECR_REGISTRY}/finops-backend:latest" >> "$ENV_FILE"
    echo "FRONTEND_IMAGE_URI=${ECR_REGISTRY}/finops-frontend:latest" >> "$ENV_FILE"
    
    # Force ECS services to pull new images
    force_ecs_deployment
}

# Force ECS services to redeploy with latest images
force_ecs_deployment() {
    log_info "Forcing ECS services to update with new Docker images..."
    
    local ecs_cluster="${STACK_NAME}-cluster"
    local backend_service="${STACK_NAME}-backend"
    local frontend_service="${STACK_NAME}-frontend"
    
    # Check if services exist
    if aws ecs describe-services \
        --cluster "$ecs_cluster" \
        --services "$backend_service" \
        --region "$AWS_REGION" \
        --query 'services[0].serviceName' \
        --output text 2>/dev/null | grep -q "$backend_service"; then
        
        log_info "Forcing backend service deployment..."
        if aws ecs update-service \
            --cluster "$ecs_cluster" \
            --service "$backend_service" \
            --force-new-deployment \
            --region "$AWS_REGION" >/dev/null 2>&1; then
            log_success "✓ Backend service forced to redeploy"
        else
            log_warning "⚠️  Backend service update failed (service may not exist yet)"
        fi
    else
        log_info "Backend service not found yet - will be created during service deployment"
    fi
    
    if aws ecs describe-services \
        --cluster "$ecs_cluster" \
        --services "$frontend_service" \
        --region "$AWS_REGION" \
        --query 'services[0].serviceName' \
        --output text 2>/dev/null | grep -q "$frontend_service"; then
        
        log_info "Forcing frontend service deployment..."
        if aws ecs update-service \
            --cluster "$ecs_cluster" \
            --service "$frontend_service" \
            --force-new-deployment \
            --region "$AWS_REGION" >/dev/null 2>&1; then
            log_success "✓ Frontend service forced to redeploy"
        else
            log_warning "⚠️  Frontend service update failed (service may not exist yet)"
        fi
    else
        log_info "Frontend service not found yet - will be created during service deployment"
    fi
    
    log_info "ECS services will pull and deploy new images within 2-3 minutes"
}

# Deploy ECS services
deploy_services() {
    log_info "Deploying ECS services..."
    
    # Source deployment environment
    if [ ! -f deployment.env ]; then
        log_error "deployment.env not found. This file should have been created during infrastructure deployment."
        log_error "Cannot proceed without database password and other deployment information."
        rollback_deployment "deployment.env file missing"
    fi
    
    source deployment.env
    
    # Verify critical variables
    if [ -z "${DB_PASSWORD:-}" ]; then
        log_error "DB_PASSWORD not found in deployment.env"
        rollback_deployment "DB_PASSWORD missing from deployment.env"
    fi
    
    # Set defaults for optional variables
    BEDROCK_MODEL="${BEDROCK_MODEL:-amazon.nova-premier-v1:0}"
    
    # Get stack outputs
    VPC_ID=$(aws cloudformation describe-stacks \
        --stack-name "$STACK_NAME" \
        --query 'Stacks[0].Outputs[?OutputKey==`VPCId`].OutputValue' \
        --output text)
    
    DB_ENDPOINT=$(aws cloudformation describe-stacks \
        --stack-name "$STACK_NAME" \
        --query 'Stacks[0].Outputs[?OutputKey==`DatabaseEndpoint`].OutputValue' \
        --output text)
    
    VALKEY_ENDPOINT=$(aws cloudformation describe-stacks \
        --stack-name "$STACK_NAME" \
        --query 'Stacks[0].Outputs[?OutputKey==`ValkeyEndpoint`].OutputValue' \
        --output text)
    
    ECS_CLUSTER=$(aws cloudformation describe-stacks \
        --stack-name "$STACK_NAME" \
        --query 'Stacks[0].Outputs[?OutputKey==`ECSClusterName`].OutputValue' \
        --output text)
    
    # Deploy ECS services using the service stack
    ECS_SERVICE_PARAMS=(
        ParentStackName="$STACK_NAME"
        BackendImageUri="$BACKEND_IMAGE_URI"
        FrontendImageUri="$FRONTEND_IMAGE_URI"
        DatabaseEndpoint="$DB_ENDPOINT"
        ValkeyEndpoint="$VALKEY_ENDPOINT"
        DatabasePassword="$DB_PASSWORD"
        BedrockModelId="$BEDROCK_MODEL"
        S3BucketName="$S3_BUCKET"
    )
    
    # Add CUR bucket parameters if configured
    # Use CUR_S3_BUCKET (set during deployment) or fall back to S3_BUCKET
    CUR_BUCKET="${CUR_S3_BUCKET:-$S3_BUCKET}"
    if [ -n "${CUR_BUCKET:-}" ]; then
        ECS_SERVICE_PARAMS+=(
            CurBucketName="$CUR_BUCKET"
            CurDatabase="${AWS_CUR_DATABASE:-cost_usage_db}"
            CurTable="${AWS_CUR_TABLE:-cur_data}"
            CurS3Prefix="${CUR_S3_PREFIX:-cost-exports/finops-cost-export}"
        )
    else
        # Use S3 bucket as fallback for CurBucketName (required parameter)
        ECS_SERVICE_PARAMS+=(
            CurBucketName="$S3_BUCKET"
            CurDatabase="cost_usage_db"
            CurTable="${AWS_CUR_TABLE:-cur_data}"
            CurS3Prefix="cost-exports/finops-cost-export"
        )
    fi
    
    # Add CertificateArn if available
    CERT_ARN=$(aws cloudformation describe-stacks \
        --stack-name "$STACK_NAME" \
        --region "$AWS_REGION" \
        --query 'Stacks[0].Outputs[?OutputKey==`CertificateArn`].OutputValue' \
        --output text 2>/dev/null || echo "")
    
    if [ -n "$CERT_ARN" ] && [ "$CERT_ARN" != "None" ]; then
        log_info "Using HTTPS with certificate: $CERT_ARN"
        ECS_SERVICE_PARAMS+=(
            CertificateArn="$CERT_ARN"
        )
    else
        log_info "No certificate found - using HTTP only"
    fi
    
    log_info "[DEBUG] Services stack BedrockModelId: $BEDROCK_MODEL"
    
    run_with_retry 3 aws cloudformation deploy \
        --template-file infrastructure/cloudformation/ecs-services.yaml \
        --stack-name "${STACK_NAME}-services" \
        --parameter-overrides "${ECS_SERVICE_PARAMS[@]}" \
        --capabilities CAPABILITY_IAM \
        --region "$AWS_REGION"
    
    if [ $? -eq 0 ]; then
        log_success "ECS services deployed successfully."
        
        # Verify critical configurations
        log_info "Verifying backend configuration..."
        
        # Check that backend task definition has ALLOWED_ORIGINS set
        TASK_DEF_ARN=$(aws ecs describe-services \
            --cluster "${STACK_NAME}-cluster" \
            --services "${STACK_NAME}-backend" \
            --region "$AWS_REGION" \
            --query 'services[0].taskDefinition' \
            --output text 2>/dev/null)
        
        if [ -n "$TASK_DEF_ARN" ]; then
            HAS_CORS=$(aws ecs describe-task-definition \
                --task-definition "$TASK_DEF_ARN" \
                --region "$AWS_REGION" \
                --query 'taskDefinition.containerDefinitions[0].environment[?name==`ALLOWED_ORIGINS`].value' \
                --output text 2>/dev/null)
            
            if [ -n "$HAS_CORS" ]; then
                log_success "✓ CORS configuration verified: ALLOWED_ORIGINS is set"
            else
                log_warning "⚠️  ALLOWED_ORIGINS not found in backend configuration"
                log_warning "    Frontend may experience CORS issues"
            fi
            
            # Verify database password is set
            HAS_DB_PWD=$(aws ecs describe-task-definition \
                --task-definition "$TASK_DEF_ARN" \
                --region "$AWS_REGION" \
                --query 'taskDefinition.containerDefinitions[0].environment[?name==`POSTGRES_PASSWORD`].value' \
                --output text 2>/dev/null)
            
            if [ -n "$HAS_DB_PWD" ]; then
                log_success "✓ Database password configured in backend"
            else
                log_error "✗ Database password not found in backend configuration"
            fi
        fi
    else
        log_error "ECS services deployment failed."
        exit 1
    fi
}

# Validate migration state
validate_migration_state() {
    log_info "Checking Alembic migration status..."
    
    local ecs_cluster backend_service
    
    ecs_cluster=$(aws cloudformation describe-stacks \
        --stack-name "$STACK_NAME" \
        --region "$AWS_REGION" \
        --query 'Stacks[0].Outputs[?OutputKey==`ECSClusterName`].OutputValue' \
        --output text 2>/dev/null || echo "")
    
    backend_service=$(aws cloudformation describe-stacks \
        --stack-name "${STACK_NAME}-services" \
        --region "$AWS_REGION" \
        --query 'Stacks[0].Outputs[?OutputKey==`BackendServiceName`].OutputValue' \
        --output text 2>/dev/null || echo "${STACK_NAME}-backend")
    
    if [ -z "$ecs_cluster" ]; then\n        log_warning "Cannot validate - ECS cluster not found"
        return 1
    fi
    
    # Try to get migration status from running task
    local task_arn=$(aws ecs list-tasks \
        --region "$AWS_REGION" \
        --cluster "$ecs_cluster" \
        --service-name "$backend_service" \
        --desired-status RUNNING \
        --query 'taskArns[0]' \
        --output text 2>/dev/null || echo "None")
    
    if [ -n "$task_arn" ] && [ "$task_arn" != "None" ] && command -v session-manager-plugin &>/dev/null; then
        # Try to check migration version via ECS Exec
        if aws ecs execute-command \
            --region "$AWS_REGION" \
            --cluster "$ecs_cluster" \
            --task "$task_arn" \
            --container backend \
            --command "alembic -c /app/alembic.ini current" \
            --non-interactive &>/dev/null; then
            return 0
        fi
    fi
    
    # If we can't validate, return success (non-blocking)
    return 0
}

# Run Alembic migrations on AWS via ECS after services are deployed
run_migrations_ecs() {
    log_info "Executing Alembic migrations on ECS..."

    # Discover ECS cluster and backend service names from CloudFormation outputs
    local ecs_cluster backend_service task_def subnets sgs task_arn

    ecs_cluster=$(aws cloudformation describe-stacks \
        --stack-name "$STACK_NAME" \
        --region "$AWS_REGION" \
        --query 'Stacks[0].Outputs[?OutputKey==`ECSClusterName`].OutputValue' \
        --output text 2>/dev/null || echo "")

    backend_service=$(aws cloudformation describe-stacks \
        --stack-name "${STACK_NAME}-services" \
        --region "$AWS_REGION" \
        --query 'Stacks[0].Outputs[?OutputKey==`BackendServiceName`].OutputValue' \
        --output text 2>/dev/null || echo "${STACK_NAME}-backend")

    if [ -z "$ecs_cluster" ] || [ "$ecs_cluster" = "None" ]; then
        log_error "Could not determine ECS cluster name from stack outputs."
        return 1
    fi

    # Try to exec into a running backend task first
    task_arn=$(aws ecs list-tasks \
        --region "$AWS_REGION" \
        --cluster "$ecs_cluster" \
        --service-name "$backend_service" \
        --desired-status RUNNING \
        --query 'taskArns[0]' \
        --output text 2>/dev/null || echo "None")

    if [ -n "$task_arn" ] && [ "$task_arn" != "None" ]; then
        # Preflight: ensure Session Manager Plugin is available locally for ECS Exec
        if ! command -v session-manager-plugin >/dev/null 2>&1; then
            log_warning "Session Manager Plugin not found locally. Skipping ECS Exec and using one-off Fargate task."
            log_warning "Install on macOS: brew install session-manager-plugin (then re-run to use ECS Exec)"
        else
            log_info "Found running backend task. Executing migrations via ECS Exec..."
            chmod +x scripts/deployment/aws_run_migrations.sh || true
            if scripts/deployment/aws_run_migrations.sh exec \
                --region "$AWS_REGION" \
                --cluster "$ecs_cluster" \
                --service "$backend_service" \
                --container backend; then
                log_success "Alembic migrations completed via ECS Exec."
                return 0
            else
                log_warning "ECS Exec migration attempt failed. Will try one-off Fargate task."
            fi
        fi
    else
        log_info "No running backend task found yet. Using one-off Fargate task to run migrations."
    fi

    # Fallback: run a one-off task with backend task definition
    task_def=$(aws ecs describe-services \
        --cluster "$ecs_cluster" \
        --services "$backend_service" \
        --region "$AWS_REGION" \
        --query 'services[0].taskDefinition' \
        --output text 2>/dev/null || echo "")

    # Resolve networking config from main stack outputs
    local subnet1 subnet2 sg
    subnet1=$(aws cloudformation describe-stacks --stack-name "$STACK_NAME" --region "$AWS_REGION" \
        --query 'Stacks[0].Outputs[?OutputKey==`PrivateSubnet1Id`].OutputValue' --output text 2>/dev/null || echo "")
    subnet2=$(aws cloudformation describe-stacks --stack-name "$STACK_NAME" --region "$AWS_REGION" \
        --query 'Stacks[0].Outputs[?OutputKey==`PrivateSubnet2Id`].OutputValue' --output text 2>/dev/null || echo "")
    sg=$(aws cloudformation describe-stacks --stack-name "$STACK_NAME" --region "$AWS_REGION" \
        --query 'Stacks[0].Outputs[?OutputKey==`ECSSecurityGroupId`].OutputValue' --output text 2>/dev/null || echo "")

    if [ -z "$task_def" ] || [ -z "$subnet1" ] || [ -z "$subnet2" ] || [ -z "$sg" ]; then
        log_error "Missing parameters for one-off task migration (task def or networking)."
        return 1
    fi

    subnets="${subnet1},${subnet2}"
    sgs="$sg"

    chmod +x scripts/deployment/aws_run_migrations.sh || true
    if scripts/deployment/aws_run_migrations.sh run \
        --region "$AWS_REGION" \
        --cluster "$ecs_cluster" \
        --task-def "$task_def" \
        --subnets "$subnets" \
        --security-groups "$sgs"; then
        log_success "Alembic migrations started via one-off Fargate task."
        return 0
    else
        log_error "Failed to start migrations via one-off Fargate task."
        return 1
    fi
}

# Get deployment information
get_deployment_info() {
    log_info "Getting deployment information..."
    
    # Set defaults for variables that may not be set in update mode
    CUR_CONFIGURED="${CUR_CONFIGURED:-false}"
    AWS_CUR_DATABASE="${AWS_CUR_DATABASE:-cost_usage_db}"
    AWS_CUR_TABLE="${AWS_CUR_TABLE:-cur_data}"
    
    # Check for custom domain first
    CUSTOM_DOMAIN=""
    APP_URL=""
    
    if [ -f "$ENV_FILE" ]; then
        CUSTOM_DOMAIN=$(grep "^DOMAIN_NAME=" "$ENV_FILE" | cut -d'=' -f2 2>/dev/null || echo "")
    fi
    
    if [ -n "$CUSTOM_DOMAIN" ]; then
        # Check if certificate exists for HTTPS
        CERT_ARN=$(aws cloudformation describe-stacks --stack-name "$STACK_NAME" --region "$AWS_REGION" --query 'Stacks[0].Outputs[?OutputKey==`CertificateArn`].OutputValue' --output text 2>/dev/null || echo "")
        if [ -n "$CERT_ARN" ] && [ "$CERT_ARN" != "None" ]; then
            APP_URL="https://$CUSTOM_DOMAIN"
        else
            APP_URL="http://$CUSTOM_DOMAIN"
        fi
    else
        # Fall back to ALB DNS
        ALB_DNS=$(aws cloudformation describe-stacks \
            --stack-name "$STACK_NAME" \
            --query 'Stacks[0].Outputs[?OutputKey==`LoadBalancerDNS`].OutputValue' \
            --output text)
        APP_URL="http://$ALB_DNS"
    fi
    
    echo ""
    log_success "Deployment completed successfully!"
    echo ""
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    echo "  📊 FinOps AI Cost Intelligence Platform"
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    echo ""
    echo "🌐 Application URL: $APP_URL"
    if [ -n "$CUSTOM_DOMAIN" ]; then
        echo "🔗 Custom Domain: $CUSTOM_DOMAIN"
        [ -n "$CERT_ARN" ] && [ "$CERT_ARN" != "None" ] && echo "🔒 HTTPS: Enabled"
    fi
    echo "📋 Stack Name: $STACK_NAME"
    echo "🏷️  Environment: $ENVIRONMENT"
    echo "📍 Region: $AWS_REGION"
    echo ""
    echo "⚠️  Note: Services may take 2-3 minutes to become available."
    if [ -n "$CUSTOM_DOMAIN" ]; then
        echo "📖 DNS propagation may take a few minutes for the custom domain."
    fi
    echo ""
    
    # Check if CUR is configured
    if [ "$CUR_CONFIGURED" = "true" ]; then
        echo "✅ CUR Data: Configured and ready"
        echo "   Database: $AWS_CUR_DATABASE"
        echo "   Table: $AWS_CUR_TABLE"
        echo "   Historical data available for analysis"
    else
        echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
        echo "  📋 Next Step: Configure CUR for Historical Data (36 months)"
        echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
        echo ""
        echo "✅ Athena table created: $AWS_CUR_DATABASE.$AWS_CUR_TABLE"
        echo "✅ Application fully functional using Cost Explorer API (13 months)"
        echo ""
        echo "To enable historical analysis beyond 13 months:"
        echo ""
        echo "1️⃣  Configure Legacy CUR in AWS Billing Console"
        echo "   → Go to: https://console.aws.amazon.com/billing/home#/reports"
        echo "   → Click: Create report"
        echo "   → Settings:"
        echo "      • Report name: finops-cost-report"
        echo "      • Time granularity: Hourly"
        echo "      • Include resource IDs: ✓ Yes"
        echo "      • Enable data integration: ✓ Amazon Athena"
        echo "      • Compression: Parquet"
        echo "      • S3 bucket: $S3_BUCKET"
        echo "      • Report path prefix: cur/finops-cost-report"
        echo ""
        echo "2️⃣  Request 36-month Historical Backfill (via AWS Support)"
        echo "   → Open AWS Support ticket: Billing category"
        echo "   → Request: \"CUR historical data backfill for 36 months\""
        echo "   → Include: CUR name and S3 bucket details"
        echo "   → Processing time: 24-72 hours"
        echo ""
        echo "3️⃣  Wait for CUR Data Delivery (24-72 hours)"
        echo "   → First report delivery: ~24 hours after CUR creation"
        echo "   → Historical backfill: 24-72 hours if requested"
        echo "   → Verify data: aws s3 ls s3://$S3_BUCKET/cur/finops-cost-report/"
        echo ""
        echo "✅ Once data arrives, it's automatically queryable!"
        echo "   • No additional scripts to run"
        echo "   • Partition projection auto-discovers data"
        echo "   • Application will use CUR data instead of Cost Explorer"
        echo "   • Saves \$0.44/month (no Glue Crawler needed)"
        echo ""
        echo "📖 Detailed guide: docs/SETUP_CUR.md"
        echo "📖 Sample queries: docs/sample_queries.sql"
        echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    fi
    echo ""
    echo "📖 Check the ECS console for service status: https://console.aws.amazon.com/ecs/"
    echo ""
}

# Validate deployment configuration
validate_deployment() {
    log_info "Running post-deployment validation..."
    echo ""
    
    local validation_passed=true
    
    # 1. Check deployment.env exists
    if [ ! -f deployment.env ]; then
        log_error "✗ deployment.env not found (should have been created during deployment)"
        validation_passed=false
    else
        log_success "✓ deployment.env exists"
        
        # Check critical values in deployment.env
        if grep -q "DB_PASSWORD=" deployment.env && [ -n "$(grep DB_PASSWORD= deployment.env | cut -d'=' -f2-)" ]; then
            log_success "✓ Database password saved in deployment.env"
        else
            log_error "✗ Database password missing from deployment.env"
            validation_passed=false
        fi
    fi
    
    # 2. Wait for backend task to be running
    log_info "Waiting for backend service to start (30 seconds)..."
    sleep 30
    
    # 3. Check backend logs for database connection
    log_info "Checking backend database connectivity..."
    DB_LOGS=$(aws logs tail /ecs/finops-intelligence-platform/backend --since 2m --region "$AWS_REGION" --format short 2>/dev/null | grep -i "database" | tail -5 || echo "")
    
    if echo "$DB_LOGS" | grep -q "password authentication failed"; then
        log_error "✗ Backend cannot connect to database (password authentication failed)"
        log_error "  This indicates a password mismatch between RDS and backend service"
        validation_passed=false
    elif echo "$DB_LOGS" | grep -q "Database.*initialized\|Database service initialized"; then
        log_success "✓ Backend connected to database successfully"
    else
        log_warning "⚠ Could not verify database connection (check logs manually)"
    fi
    
    # 4. Check for CORS configuration
    log_info "Verifying CORS configuration..."
    if [ -f infrastructure/cloudformation/ecs-services.yaml ]; then
        if grep -q "ALLOWED_ORIGINS" infrastructure/cloudformation/ecs-services.yaml; then
            log_success "✓ CORS configuration present in template"
        else
            log_error "✗ ALLOWED_ORIGINS not found in ECS services template"
            validation_passed=false
        fi
    fi
    
    echo ""
    if [ "$validation_passed" = true ]; then
        log_success "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
        log_success "  ✅ All validation checks passed!"
        log_success "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
        return 0
    else
        log_warning "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
        log_warning "  ⚠️  Some validation checks failed"
        log_warning "  Review the errors above and run fix scripts if needed:"
        log_warning "    - ./sync-db-password.sh (for database issues)"
        log_warning "    - ./fix-cors-issue.sh (for CORS issues)"
        log_warning "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
        return 1
    fi
}

# Print deployment summary
print_deployment_summary() {
    echo ""
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    echo "                    DEPLOYMENT SUMMARY"
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    echo ""
    
    # Check CloudFormation stacks
    MAIN_STACK_STATUS=$(aws cloudformation describe-stacks --stack-name "$STACK_NAME" --region "$AWS_REGION" --query 'Stacks[0].StackStatus' --output text 2>/dev/null || echo "NOT_FOUND")
    SERVICE_STACK_STATUS=$(aws cloudformation describe-stacks --stack-name "${STACK_NAME}-services" --region "$AWS_REGION" --query 'Stacks[0].StackStatus' --output text 2>/dev/null || echo "NOT_FOUND")
    
    # Check ECR repositories (suppress stdout to avoid JSON contaminating the status variable)
    if aws ecr describe-repositories --repository-names finops-backend --region "$AWS_REGION" >/dev/null 2>&1; then
        BACKEND_ECR="EXISTS"
    else
        BACKEND_ECR="NOT_FOUND"
    fi
    if aws ecr describe-repositories --repository-names finops-frontend --region "$AWS_REGION" >/dev/null 2>&1; then
        FRONTEND_ECR="EXISTS"
    else
        FRONTEND_ECR="NOT_FOUND"
    fi
    
    # Check S3 bucket
    S3_BUCKET_STATUS="NOT_FOUND"
    if [ -f "$ENV_FILE" ]; then
        S3_BUCKET=$(grep "^S3_BUCKET=" "$ENV_FILE" | cut -d'=' -f2)
        if [ -n "$S3_BUCKET" ] && aws s3 ls "s3://$S3_BUCKET" &>/dev/null; then
            S3_BUCKET_STATUS="EXISTS: $S3_BUCKET"
        fi
    fi
    
    # Check CUR configuration
    CUR_STATUS="NOT_CONFIGURED"
    EXPORT_NAME=""
    if [ -f "$ENV_FILE" ]; then
        # Grep may return non-zero when the key is missing; under 'set -euo pipefail' this would abort.
        # Use a tolerant pattern (|| true) so missing optional keys don't fail the script.
        CUR_BUCKET=$( (grep "^CUR_S3_BUCKET=" "$ENV_FILE" || true) | cut -d'=' -f2 | tr -d '"')
        EXPORT_NAME=$( (grep "^EXPORT_NAME=" "$ENV_FILE" || true) | cut -d'=' -f2 | tr -d '"')
        if [ -n "$EXPORT_NAME" ] && aws cur describe-report-definitions --region us-east-1 --query "ReportDefinitions[?ReportName=='$EXPORT_NAME']" --output text 2>/dev/null | grep -q "$EXPORT_NAME"; then
            CUR_STATUS="CONFIGURED: $EXPORT_NAME"
        else
            CUR_STATUS="CONFIGURED (bucket only, no CUR report)"
        fi
    fi
    
    # Check Glue resources (suppress stdout similarly)
    if aws glue get-database --name cost_usage_db --region "$AWS_REGION" >/dev/null 2>&1; then
        GLUE_DB_STATUS="EXISTS"
    else
        GLUE_DB_STATUS="NOT_FOUND"
    fi
    if aws athena get-work-group --work-group finops-workgroup --region "$AWS_REGION" >/dev/null 2>&1; then
        ATHENA_WG_STATUS="EXISTS"
    else
        ATHENA_WG_STATUS="NOT_FOUND"
    fi
    
    # Print summary
    echo "📦 Infrastructure Resources:"
    if [ "$MAIN_STACK_STATUS" = "CREATE_COMPLETE" ] || [ "$MAIN_STACK_STATUS" = "UPDATE_COMPLETE" ]; then
        log_success "CloudFormation Main Stack: $MAIN_STACK_STATUS"
    else
        log_error "CloudFormation Main Stack: $MAIN_STACK_STATUS"
    fi
    
    if [ "$SERVICE_STACK_STATUS" = "CREATE_COMPLETE" ] || [ "$SERVICE_STACK_STATUS" = "UPDATE_COMPLETE" ]; then
        log_success "CloudFormation Services Stack: $SERVICE_STACK_STATUS"
    else
        log_error "CloudFormation Services Stack: $SERVICE_STACK_STATUS"
    fi
    
    echo ""
    echo "🐳 Container Resources:"
    [ "$BACKEND_ECR" = "EXISTS" ] && log_success "Backend ECR Repository: EXISTS" || log_error "Backend ECR Repository: $BACKEND_ECR"
    [ "$FRONTEND_ECR" = "EXISTS" ] && log_success "Frontend ECR Repository: EXISTS" || log_error "Frontend ECR Repository: $FRONTEND_ECR"
    
    echo ""
    echo "💾 Data Resources:"
    if [ "$S3_BUCKET_STATUS" != "NOT_FOUND" ]; then
        log_success "S3 Bucket: $S3_BUCKET_STATUS"
    else
        log_error "S3 Bucket: $S3_BUCKET_STATUS"
    fi
    
    if [ "$CUR_STATUS" = "CONFIGURED: $EXPORT_NAME" ]; then
        log_success "CUR Report: $CUR_STATUS"
    elif [ "$CUR_STATUS" = "NOT_CONFIGURED" ]; then
        log_warning "CUR Report: $CUR_STATUS (using Cost Explorer API)"
    else
        log_warning "CUR Report: $CUR_STATUS"
    fi
    
    [ "$GLUE_DB_STATUS" = "EXISTS" ] && log_success "Glue Database: $GLUE_DB_STATUS" || log_warning "Glue Database: $GLUE_DB_STATUS"
    [ "$ATHENA_WG_STATUS" = "EXISTS" ] && log_success "Athena Workgroup: $ATHENA_WG_STATUS" || log_warning "Athena Workgroup: $ATHENA_WG_STATUS"
    
    echo ""
    echo "🌐 Application Access:"
    
    # Check for custom domain configuration
    CUSTOM_DOMAIN=""
    ROUTE53_STATUS="NOT_CONFIGURED"
    CERT_STATUS="NOT_CONFIGURED"
    
    if [ -f "$ENV_FILE" ]; then
        CUSTOM_DOMAIN=$(grep "^DOMAIN_NAME=" "$ENV_FILE" | cut -d'=' -f2)
        HOSTED_ZONE=$(grep "^HOSTED_ZONE_ID=" "$ENV_FILE" | cut -d'=' -f2)
        
        if [ -n "$CUSTOM_DOMAIN" ] && [ -n "$HOSTED_ZONE" ]; then
            # Check if Route 53 record exists (use JSON output for accurate checking)
            RECORD_JSON=$(aws route53 list-resource-record-sets --hosted-zone-id "$HOSTED_ZONE" --query "ResourceRecordSets[?Name=='${CUSTOM_DOMAIN}.']" --output json 2>/dev/null || echo "[]")
            RECORD_COUNT=$(echo "$RECORD_JSON" | jq 'length' 2>/dev/null || echo "0")
            
            if [ "$RECORD_COUNT" -gt 0 ]; then
                ROUTE53_STATUS="CONFIGURED"
                log_success "Route 53 DNS Record: $CUSTOM_DOMAIN → ALB"
                
                # Check for ACM certificate
                CERT_ARN=$(aws cloudformation describe-stacks --stack-name "$STACK_NAME" --region "$AWS_REGION" --query 'Stacks[0].Outputs[?OutputKey==`CertificateArn`].OutputValue' --output text 2>/dev/null || echo "")
                if [ -n "$CERT_ARN" ] && [ "$CERT_ARN" != "None" ]; then
                    CERT_STATUS="CONFIGURED"
                    log_success "ACM Certificate: Configured (HTTPS enabled)"
                    APP_URL="https://$CUSTOM_DOMAIN"
                else
                    log_warning "ACM Certificate: Not configured (using HTTP)"
                    APP_URL="http://$CUSTOM_DOMAIN"
                fi
            else
                # DNS record not found - this is expected if custom domain setup is incomplete
                ROUTE53_STATUS="NOT_CONFIGURED"
                log_warning "Route 53 DNS Record: Not configured for $CUSTOM_DOMAIN"
                log_info "To set up custom domain, see: docs/CUSTOM_DOMAIN_CONFIG.md"
                # Fall back to ALB
                ALB_DNS=$(aws cloudformation describe-stacks --stack-name "$STACK_NAME" --region "$AWS_REGION" --query 'Stacks[0].Outputs[?OutputKey==`LoadBalancerDNS`].OutputValue' --output text 2>/dev/null || echo "NOT_AVAILABLE")
                APP_URL="http://$ALB_DNS"
                log_info "Using ALB URL: $APP_URL"
            fi
        fi
    fi
    
    # If no custom domain, use ALB DNS
    if [ -z "$CUSTOM_DOMAIN" ]; then
        ALB_DNS=$(aws cloudformation describe-stacks --stack-name "$STACK_NAME" --region "$AWS_REGION" --query 'Stacks[0].Outputs[?OutputKey==`LoadBalancerDNS`].OutputValue' --output text 2>/dev/null || echo "NOT_AVAILABLE")
        APP_URL="http://$ALB_DNS"
        log_info "Using ALB DNS (no custom domain configured)"
    fi
    
    if [ "$APP_URL" != "http://NOT_AVAILABLE" ]; then
        echo ""
        log_success "🌍 Application URL: $APP_URL"
    else
        log_error "Application URL: NOT_AVAILABLE"
    fi
    
    echo ""
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    
    # Overall status
    if [ "$MAIN_STACK_STATUS" = "CREATE_COMPLETE" ] || [ "$MAIN_STACK_STATUS" = "UPDATE_COMPLETE" ]; then
        if [ "$SERVICE_STACK_STATUS" = "CREATE_COMPLETE" ] || [ "$SERVICE_STACK_STATUS" = "UPDATE_COMPLETE" ]; then
            log_success "✅ Deployment completed successfully!"
            echo ""
            log_info "Next steps:"
            echo "  1. Access the application at: $APP_URL"
            if [ "$CERT_STATUS" = "CONFIGURED" ]; then
                echo "     (HTTPS enabled with custom domain)"
            elif [ -n "$CUSTOM_DOMAIN" ]; then
                echo "     (Custom domain without HTTPS)"
            fi
            echo "  2. Services may take 2-3 minutes to become available"
            if [ "$CUR_STATUS" = "NOT_CONFIGURED" ]; then
                echo "  3. Platform is using Cost Explorer API (13 months of data)"
                echo "  4. To enable CUR for extended history, run: scripts/setup/setup-cur.sh"
            else
                echo "  3. CUR data will be available in 24 hours"
                echo "  4. Run Glue crawler after CUR data arrives"
            fi
        else
            log_error "⚠️  Deployment completed with issues (services stack failed)"
        fi
    else
        log_error "❌ Deployment failed (main stack error)"
    fi
    echo ""
}

# Print destroy summary
print_destroy_summary() {
    echo ""
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    echo "                    DESTRUCTION SUMMARY"
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    echo ""
    
    # Check what was deleted
    MAIN_STACK_EXISTS=$(aws cloudformation describe-stacks --stack-name "$STACK_NAME" --region "$AWS_REGION" 2>/dev/null && echo "STILL_EXISTS" || echo "DELETED")
    SERVICE_STACK_EXISTS=$(aws cloudformation describe-stacks --stack-name "${STACK_NAME}-services" --region "$AWS_REGION" 2>/dev/null && echo "STILL_EXISTS" || echo "DELETED")
    BACKEND_ECR_EXISTS=$(aws ecr describe-repositories --repository-names finops-backend --region "$AWS_REGION" 2>/dev/null && echo "STILL_EXISTS" || echo "DELETED")
    FRONTEND_ECR_EXISTS=$(aws ecr describe-repositories --repository-names finops-frontend --region "$AWS_REGION" 2>/dev/null && echo "STILL_EXISTS" || echo "DELETED")
    
    # Check what was preserved
    CUR_EXISTS=$(aws cur describe-report-definitions --region us-east-1 --query 'ReportDefinitions[?contains(ReportName, `finops`)]' --output text 2>/dev/null | wc -l | tr -d ' ')
    S3_BUCKETS=$(aws s3api list-buckets --query "Buckets[?contains(Name, 'finops')].Name" --output text 2>/dev/null | wc -w || echo "0")
    GLUE_DB_EXISTS=$(aws glue get-database --name cost_usage_db --region "$AWS_REGION" 2>/dev/null && echo "PRESERVED" || echo "DELETED")
    ATHENA_WG_EXISTS=$(aws athena get-work-group --work-group finops-workgroup --region "$AWS_REGION" 2>/dev/null && echo "PRESERVED" || echo "DELETED")
    
    echo "🗑️  Deleted Resources:"
    [ "$MAIN_STACK_EXISTS" = "DELETED" ] && log_success "CloudFormation Main Stack: DELETED" || log_error "CloudFormation Main Stack: STILL EXISTS"
    [ "$SERVICE_STACK_EXISTS" = "DELETED" ] && log_success "CloudFormation Services Stack: DELETED" || log_error "CloudFormation Services Stack: STILL EXISTS"
    [ "$BACKEND_ECR_EXISTS" = "DELETED" ] && log_success "Backend ECR Repository: DELETED" || log_error "Backend ECR Repository: STILL EXISTS"
    [ "$FRONTEND_ECR_EXISTS" = "DELETED" ] && log_success "Frontend ECR Repository: DELETED" || log_error "Frontend ECR Repository: STILL EXISTS"
    
    echo ""
    echo "💾 Preserved Resources:"
    if [ "$CUR_EXISTS" -gt 0 ]; then
        log_info "CUR Reports: $CUR_EXISTS report(s) preserved"
    else
        log_info "CUR Reports: None found"
    fi
    
    if [ "$S3_BUCKETS" -gt 0 ]; then
        log_info "S3 Buckets: $S3_BUCKETS bucket(s) preserved"
        aws s3api list-buckets --query "Buckets[?contains(Name, 'finops')].Name" --output text 2>/dev/null | tr '\t' '\n' | sed 's/^/    - /'
    else
        log_info "S3 Buckets: None found"
    fi
    
    [ "$GLUE_DB_EXISTS" = "PRESERVED" ] && log_info "Glue Database: PRESERVED" || log_info "Glue Database: Not found"
    [ "$ATHENA_WG_EXISTS" = "PRESERVED" ] && log_info "Athena Workgroup: PRESERVED" || log_info "Athena Workgroup: Not found"
    
    echo ""
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    
    if [ "$MAIN_STACK_EXISTS" = "DELETED" ] && [ "$SERVICE_STACK_EXISTS" = "DELETED" ]; then
        log_success "✅ Infrastructure destroyed successfully!"
        echo ""
        log_info "Preserved resources for re-deployment:"
        echo "  • CUR/Data Export configurations"
        echo "  • S3 buckets with historical cost data"
        echo "  • Glue databases and tables"
        echo "  • Athena workgroups"
        echo ""
        log_info "To completely remove all resources, run: ./deploy.sh destroyAll"
    else
        log_error "⚠️  Destruction completed with issues"
        echo ""
        log_info "Some resources may still exist. Check AWS Console or re-run destroy."
    fi
    echo ""
}

# Print complete destruction summary
print_destroyAll_summary() {
    echo ""
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    echo "                 COMPLETE DESTRUCTION SUMMARY"
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    echo ""
    
    # Check everything
    MAIN_STACK_EXISTS=$(aws cloudformation describe-stacks --stack-name "$STACK_NAME" --region "$AWS_REGION" 2>/dev/null && echo "STILL_EXISTS" || echo "DELETED")
    SERVICE_STACK_EXISTS=$(aws cloudformation describe-stacks --stack-name "${STACK_NAME}-services" --region "$AWS_REGION" 2>/dev/null && echo "STILL_EXISTS" || echo "DELETED")
    BACKEND_ECR_EXISTS=$(aws ecr describe-repositories --repository-names finops-backend --region "$AWS_REGION" 2>/dev/null && echo "STILL_EXISTS" || echo "DELETED")
    FRONTEND_ECR_EXISTS=$(aws ecr describe-repositories --repository-names finops-frontend --region "$AWS_REGION" 2>/dev/null && echo "STILL_EXISTS" || echo "DELETED")
    CUR_COUNT=$(aws cur describe-report-definitions --region us-east-1 --query 'ReportDefinitions[?contains(ReportName, `finops`)]' --output text 2>/dev/null | wc -l | tr -d ' ')
    S3_BUCKET_COUNT=$(aws s3api list-buckets --query "Buckets[?contains(Name, 'finops')].Name" --output text 2>/dev/null | wc -w || echo "0")
    GLUE_DB_EXISTS=$(aws glue get-database --name cost_usage_db --region "$AWS_REGION" 2>/dev/null && echo "STILL_EXISTS" || echo "DELETED")
    ATHENA_WG_EXISTS=$(aws athena get-work-group --work-group finops-workgroup --region "$AWS_REGION" 2>/dev/null && echo "STILL_EXISTS" || echo "DELETED")
    SSM_PARAM_COUNT=$(aws ssm describe-parameters --region "$AWS_REGION" --parameter-filters "Key=Name,Option=Contains,Values=/${STACK_NAME}/" --query 'Parameters' --output text 2>/dev/null | wc -l | tr -d ' ')
    GLUE_ROLE_EXISTS=$(aws iam get-role --role-name "AWSGlueServiceRole-FinOps" 2>/dev/null && echo "STILL_EXISTS" || echo "DELETED")
    
    # Network resources
    VPC_COUNT=$(aws ec2 describe-vpcs --region "$AWS_REGION" --filters "Name=tag:Name,Values=*${STACK_NAME}*" --query 'length(Vpcs)' --output text 2>/dev/null || echo "0")
    SG_COUNT=$(aws ec2 describe-security-groups --region "$AWS_REGION" --filters "Name=group-name,Values=${STACK_NAME}*" --query 'length(SecurityGroups)' --output text 2>/dev/null || echo "0")
    ENI_COUNT=$(aws ec2 describe-network-interfaces --region "$AWS_REGION" --filters "Name=tag:Name,Values=*${STACK_NAME}*" --query 'length(NetworkInterfaces)' --output text 2>/dev/null || echo "0")
    NAT_COUNT=$(aws ec2 describe-nat-gateways --region "$AWS_REGION" --filter "Name=tag:Name,Values=*${STACK_NAME}*" "Name=state,Values=available,pending,deleting" --query 'length(NatGateways)' --output text 2>/dev/null || echo "0")
    EIP_COUNT=$(aws ec2 describe-addresses --region "$AWS_REGION" --filters "Name=tag:Name,Values=*${STACK_NAME}*" --query 'length(Addresses)' --output text 2>/dev/null || echo "0")
    
    echo "🗑️  Resource Deletion Status:"
    echo ""
    
    # CloudFormation
    [ "$MAIN_STACK_EXISTS" = "DELETED" ] && log_success "CloudFormation Main Stack: DELETED ✓" || log_error "CloudFormation Main Stack: STILL EXISTS ✗"
    [ "$SERVICE_STACK_EXISTS" = "DELETED" ] && log_success "CloudFormation Services Stack: DELETED ✓" || log_error "CloudFormation Services Stack: STILL EXISTS ✗"
    
    # ECR
    [ "$BACKEND_ECR_EXISTS" = "DELETED" ] && log_success "Backend ECR Repository: DELETED ✓" || log_error "Backend ECR Repository: STILL EXISTS ✗"
    [ "$FRONTEND_ECR_EXISTS" = "DELETED" ] && log_success "Frontend ECR Repository: DELETED ✓" || log_error "Frontend ECR Repository: STILL EXISTS ✗"
    
    # Network Resources
    if [ "$VPC_COUNT" -eq 0 ]; then
        log_success "VPCs: DELETED ✓ (0 VPCs found)"
    else
        log_error "VPCs: STILL EXISTS ✗ ($VPC_COUNT VPC(s) remaining)"
        aws ec2 describe-vpcs --region "$AWS_REGION" --filters "Name=tag:Name,Values=*${STACK_NAME}*" --query 'Vpcs[].[VpcId,Tags[?Key==`Name`].Value|[0]]' --output text 2>/dev/null | sed 's/^/    - /' || true
    fi
    
    if [ "$SG_COUNT" -eq 0 ]; then
        log_success "Security Groups: DELETED ✓ (0 security groups found)"
    else
        log_error "Security Groups: STILL EXISTS ✗ ($SG_COUNT security group(s) remaining)"
        aws ec2 describe-security-groups --region "$AWS_REGION" --filters "Name=group-name,Values=${STACK_NAME}*" --query 'SecurityGroups[].[GroupId,GroupName]' --output text 2>/dev/null | sed 's/^/    - /' || true
    fi
    
    if [ "$ENI_COUNT" -eq 0 ]; then
        log_success "Network Interfaces (ENIs): DELETED ✓ (0 ENIs found)"
    else
        log_warning "Network Interfaces (ENIs): $ENI_COUNT ENI(s) remaining (may be in use)"
    fi
    
    if [ "$NAT_COUNT" -eq 0 ]; then
        log_success "NAT Gateways: DELETED ✓ (0 NAT gateways found)"
    else
        log_warning "NAT Gateways: $NAT_COUNT NAT gateway(s) still deleting (this is normal)"
    fi
    
    if [ "$EIP_COUNT" -eq 0 ]; then
        log_success "Elastic IPs: DELETED ✓ (0 EIPs found)"
    else
        log_error "Elastic IPs: STILL EXISTS ✗ ($EIP_COUNT EIP(s) remaining)"
    fi
    
    # CUR
    if [ "$CUR_COUNT" -eq 0 ]; then
        log_success "CUR Reports: DELETED ✓ (0 reports found)"
    else
        log_error "CUR Reports: STILL EXISTS ✗ ($CUR_COUNT report(s) remaining)"
    fi
    
    # S3
    if [ "$S3_BUCKET_COUNT" -eq 0 ]; then
        log_success "S3 Buckets: DELETED ✓ (0 buckets found)"
    else
        log_error "S3 Buckets: STILL EXISTS ✗ ($S3_BUCKET_COUNT bucket(s) remaining)"
        aws s3api list-buckets --query "Buckets[?contains(Name, 'finops')].Name" --output text 2>/dev/null | tr '\t' '\n' | sed 's/^/    - /' || true
    fi
    
    # Glue & Athena
    [ "$GLUE_DB_EXISTS" = "DELETED" ] && log_success "Glue Database: DELETED ✓" || log_error "Glue Database: STILL EXISTS ✗"
    [ "$ATHENA_WG_EXISTS" = "DELETED" ] && log_success "Athena Workgroup: DELETED ✓" || log_error "Athena Workgroup: STILL EXISTS ✗"
    
    # IAM Roles
    [ "$GLUE_ROLE_EXISTS" = "DELETED" ] && log_success "Glue IAM Role: DELETED ✓" || log_error "Glue IAM Role: STILL EXISTS ✗"
    
    # SSM Parameters
    if [ "$SSM_PARAM_COUNT" -eq 0 ]; then
        log_success "SSM Parameters: DELETED ✓ (0 parameters found)"
    else
        log_error "SSM Parameters: STILL EXISTS ✗ ($SSM_PARAM_COUNT parameter(s) remaining)"
    fi
    
    # CloudWatch Log Groups
    LOG_GROUP_COUNT=$(aws logs describe-log-groups --region "$AWS_REGION" --log-group-name-prefix "/ecs/$STACK_NAME" --query 'length(logGroups)' --output text 2>/dev/null || echo "0")
    if [ "$LOG_GROUP_COUNT" -eq 0 ]; then
        log_success "CloudWatch Log Groups: DELETED ✓ (0 log groups found)"
    else
        log_error "CloudWatch Log Groups: STILL EXISTS ✗ ($LOG_GROUP_COUNT log group(s) remaining)"
        aws logs describe-log-groups --region "$AWS_REGION" --log-group-name-prefix "/ecs/$STACK_NAME" --query 'logGroups[].logGroupName' --output text 2>/dev/null | tr '\t' '\n' | sed 's/^/    - /' || true
    fi
    
    echo ""
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    
    # Overall status
    FAILED_COUNT=0
    [ "$MAIN_STACK_EXISTS" = "STILL_EXISTS" ] && ((FAILED_COUNT++))
    [ "$SERVICE_STACK_EXISTS" = "STILL_EXISTS" ] && ((FAILED_COUNT++))
    [ "$CUR_COUNT" -gt 0 ] && ((FAILED_COUNT++))
    [ "$S3_BUCKET_COUNT" -gt 0 ] && ((FAILED_COUNT++))
    [ "$GLUE_DB_EXISTS" = "STILL_EXISTS" ] && ((FAILED_COUNT++))
    [ "$ATHENA_WG_EXISTS" = "STILL_EXISTS" ] && ((FAILED_COUNT++))
    [ "$GLUE_ROLE_EXISTS" = "STILL_EXISTS" ] && ((FAILED_COUNT++))
    [ "$SSM_PARAM_COUNT" -gt 0 ] && ((FAILED_COUNT++))
    [ "$LOG_GROUP_COUNT" -gt 0 ] && ((FAILED_COUNT++))
    [ "$VPC_COUNT" -gt 0 ] && ((FAILED_COUNT++))
    [ "$SG_COUNT" -gt 0 ] && ((FAILED_COUNT++))
    [ "$EIP_COUNT" -gt 0 ] && ((FAILED_COUNT++))
    
    if [ "$FAILED_COUNT" -eq 0 ]; then
        log_success "✅ Complete destruction finished successfully!"
        log_success "All FinOps resources have been permanently deleted."
        echo ""
        log_info "You can now perform a fresh deployment with: ./deploy.sh deploy"
    else
        log_error "⚠️  Destruction completed with $FAILED_COUNT issue(s)"
        echo ""
        log_warning "═══════════════════════════════════════════════════════════════════"
        log_warning "         MANUAL CLEANUP COMMANDS FOR REMAINING RESOURCES"
        log_warning "═══════════════════════════════════════════════════════════════════"
        echo ""
        
        if [ "$VPC_COUNT" -gt 0 ]; then
            echo "🔧 VPCs still exist. To delete them:"
            aws ec2 describe-vpcs --region "$AWS_REGION" --filters "Name=tag:Name,Values=*${STACK_NAME}*" --query 'Vpcs[].VpcId' --output text 2>/dev/null | tr '\t' '\n' | while read vpc; do
                echo "   aws ec2 delete-vpc --vpc-id $vpc --region $AWS_REGION"
            done
            echo ""
        fi
        
        if [ "$SG_COUNT" -gt 0 ]; then
            echo "🔧 Security Groups still exist. To delete them:"
            aws ec2 describe-security-groups --region "$AWS_REGION" --filters "Name=group-name,Values=${STACK_NAME}*" --query 'SecurityGroups[].GroupId' --output text 2>/dev/null | tr '\t' '\n' | while read sg; do
                echo "   aws ec2 delete-security-group --group-id $sg --region $AWS_REGION"
            done
            echo "   Note: Remove ingress rules first if deletion fails:"
            echo "   ./scripts/cleanup-orphaned-resources.sh"
            echo ""
        fi
        
        if [ "$ENI_COUNT" -gt 0 ]; then
            echo "🔧 Network Interfaces (ENIs) still exist. To delete available ENIs:"
            echo "   aws ec2 describe-network-interfaces --region $AWS_REGION --filters \"Name=tag:Name,Values=*${STACK_NAME}*\" --query 'NetworkInterfaces[?Status==\`available\`].NetworkInterfaceId' --output text | xargs -n1 aws ec2 delete-network-interface --network-interface-id"
            echo ""
        fi
        
        if [ "$EIP_COUNT" -gt 0 ]; then
            echo "🔧 Elastic IPs still exist. To release them:"
            aws ec2 describe-addresses --region "$AWS_REGION" --filters "Name=tag:Name,Values=*${STACK_NAME}*" --query 'Addresses[].AllocationId' --output text 2>/dev/null | tr '\t' '\n' | while read eip; do
                echo "   aws ec2 release-address --allocation-id $eip --region $AWS_REGION"
            done
            echo ""
        fi
        
        if [ "$S3_BUCKET_COUNT" -gt 0 ]; then
            echo "🔧 S3 buckets still exist. To delete them:"
            aws s3api list-buckets --query "Buckets[?contains(Name, 'finops')].Name" --output text 2>/dev/null | tr '\t' '\n' | while read bucket; do
                echo "   aws s3 rm s3://$bucket --recursive && aws s3 rb s3://$bucket --force"
            done
            echo ""
        fi
        
        if [ "$CUR_COUNT" -gt 0 ]; then
            echo "🔧 CUR Reports still exist. To delete them:"
            aws cur describe-report-definitions --region us-east-1 --query 'ReportDefinitions[?contains(ReportName, `finops`)].ReportName' --output text 2>/dev/null | tr '\t' '\n' | while read report; do
                echo "   aws cur delete-report-definition --report-name $report --region us-east-1"
            done
            echo ""
        fi
        
        if [ "$LOG_GROUP_COUNT" -gt 0 ]; then
            echo "🔧 CloudWatch Log Groups still exist. To delete them:"
            aws logs describe-log-groups --region "$AWS_REGION" --log-group-name-prefix "/ecs/$STACK_NAME" --query 'logGroups[].logGroupName' --output text 2>/dev/null | tr '\t' '\n' | while read lg; do
                echo "   aws logs delete-log-group --log-group-name $lg --region $AWS_REGION"
            done
            echo ""
        fi
        
        echo "🔧 Comprehensive cleanup script available:"
        echo "   ./scripts/cleanup-orphaned-resources.sh"
        echo ""
        log_warning "═══════════════════════════════════════════════════════════════════"
    fi
    echo ""
}

# Remove RDS delete protection
remove_rds_delete_protection() {
    log_info "Checking for RDS instances with delete protection..."
    
    # Get RDS instance identifier from stack
    DB_INSTANCE_ID=$(aws cloudformation describe-stack-resources \
        --stack-name "$STACK_NAME" \
        --query 'StackResources[?ResourceType==`AWS::RDS::DBInstance`].PhysicalResourceId' \
        --output text \
        --region "$AWS_REGION" 2>/dev/null || true)
    
    if [ -n "$DB_INSTANCE_ID" ]; then
        log_info "Found RDS instance: $DB_INSTANCE_ID"
        
        # Check if delete protection is enabled
        DELETE_PROTECTION=$(aws rds describe-db-instances \
            --db-instance-identifier "$DB_INSTANCE_ID" \
            --query 'DBInstances[0].DeletionProtection' \
            --output text \
            --region "$AWS_REGION" 2>/dev/null || true)
        
        if [ "$DELETE_PROTECTION" = "True" ]; then
            log_warning "Delete protection is enabled. Disabling it now..."
            aws rds modify-db-instance \
                --db-instance-identifier "$DB_INSTANCE_ID" \
                --no-deletion-protection \
                --apply-immediately \
                --region "$AWS_REGION"
            
            log_info "Waiting for RDS modification to complete..."
            aws rds wait db-instance-available \
                --db-instance-identifier "$DB_INSTANCE_ID" \
                --region "$AWS_REGION"
            
            log_success "Delete protection removed from RDS instance."
        else
            log_info "Delete protection is already disabled."
        fi
    else
        log_info "No RDS instances found in stack."
    fi
}

# Destroy infrastructure (CloudFormation stacks and ECR only)
destroy_infrastructure() {
    log_warning "⚠️  WARNING: This will delete CloudFormation stacks and ECR repositories!"
    echo ""
    echo "The following resources will be DELETED:"
    echo "  • CloudFormation stack: ${STACK_NAME}"
    echo "  • CloudFormation stack: ${STACK_NAME}-services"
    echo "  • ECR repositories: finops-backend, finops-frontend"
    echo "  • RDS databases (and all data)"
    echo "  • ECS clusters and tasks"
    echo "  • VPC and networking resources"
    echo ""
    echo "The following resources will NOT be deleted:"
    echo "  • CUR/Data Export reports"
    echo "  • S3 buckets (including CUR data)"
    echo "  • Glue databases and tables"
    echo "  • Athena workgroups and query results"
    echo "  • Route53 DNS records"
    echo "  • ACM SSL certificates"
    echo ""
    log_info "Use 'destroyAll' to delete everything including data exports, S3 buckets, DNS records, and certificates"
    echo ""
    read -p "Are you sure you want to continue? (yes/no): " CONFIRM
    
    if [ "$CONFIRM" != "yes" ]; then
        log_info "Destruction cancelled."
        exit 0
    fi
    
    log_info "Starting infrastructure destruction..."
    
    # Ensure required tools and credentials are available
    check_prerequisites
    
    # Load deployment metadata if present (may contain actual STACK_NAME, image URIs, etc.)
    if [ -f deployment.env ]; then
        log_info "Loading deployment metadata from deployment.env"
        # shellcheck disable=SC1091
        source deployment.env || true
    fi
    
    # Remove RDS delete protection first
    remove_rds_delete_protection
    
    # Delete services stack
    log_info "Deleting services stack..."
    aws cloudformation delete-stack --stack-name "${STACK_NAME}-services" --region "$AWS_REGION" 2>/dev/null || true
    
    # Wait for services stack deletion (with timeout)
    log_info "Waiting for services stack deletion..."
    aws cloudformation wait stack-delete-complete --stack-name "${STACK_NAME}-services" --region "$AWS_REGION" 2>/dev/null || true
    
    # Delete main stack
    log_info "Deleting main infrastructure stack..."
    aws cloudformation delete-stack --stack-name "$STACK_NAME" --region "$AWS_REGION"
    
    # Wait for main stack deletion
    log_info "Waiting for main stack deletion... (this may take several minutes)"
    aws cloudformation wait stack-delete-complete --stack-name "$STACK_NAME" --region "$AWS_REGION"
    
    # Clean up ECR repositories
    log_info "Cleaning up ECR repositories..."
    AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
    
    aws ecr delete-repository --repository-name finops-backend --force --region "$AWS_REGION" 2>/dev/null || true
    aws ecr delete-repository --repository-name finops-frontend --force --region "$AWS_REGION" 2>/dev/null || true
    
    rm -f "$ENV_FILE"
    
    # Print summary
    print_destroy_summary
}

# Destroy everything including data exports, S3 buckets, Glue, and Athena
destroy_all() {
    echo ""
    echo "======================================================================"
    echo "                    ⚠️  COMPLETE DESTRUCTION WARNING ⚠️"
    echo "======================================================================"
    echo ""
    log_error "This will DELETE EVERYTHING - NO DATA CAN BE RECOVERED!"
    echo ""
    
    # Validate AWS credentials before proceeding
    if ! validate_aws_credentials; then
        exit 1
    fi
    
    # Get AWS account ID
    AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
    
    echo "Scanning for existing resources..."
    echo ""
    
    RESOURCES_TO_DELETE=()
    
    # Check for CloudFormation stacks
    if aws cloudformation describe-stacks --stack-name "$STACK_NAME" --region "$AWS_REGION" &>/dev/null; then
        RESOURCES_TO_DELETE+=("  ✓ CloudFormation stack: ${STACK_NAME}")
    fi
    if aws cloudformation describe-stacks --stack-name "${STACK_NAME}-services" --region "$AWS_REGION" &>/dev/null; then
        RESOURCES_TO_DELETE+=("  ✓ CloudFormation stack: ${STACK_NAME}-services")
    fi
    
    # Check for ECR repositories
    if aws ecr describe-repositories --repository-names finops-backend --region "$AWS_REGION" &>/dev/null; then
        RESOURCES_TO_DELETE+=("  ✓ ECR repository: finops-backend")
    fi
    if aws ecr describe-repositories --repository-names finops-frontend --region "$AWS_REGION" &>/dev/null; then
        RESOURCES_TO_DELETE+=("  ✓ ECR repository: finops-frontend")
    fi
    
    # Check for Data Exports (new API)
    DATA_EXPORTS=$(aws bcm-data-exports list-exports --region us-east-1 --query "Exports[?contains(Export.Name, 'finops')].Export.Name" --output text 2>/dev/null || echo "")
    if [ -n "$DATA_EXPORTS" ]; then
        for export in $DATA_EXPORTS; do
            RESOURCES_TO_DELETE+=("  ✓ Data Export: $export")
        done
    fi
    
    # Check for Legacy CUR reports
    CUR_REPORTS=$(aws cur describe-report-definitions --region us-east-1 --query "ReportDefinitions[?contains(ReportName, 'finops')].ReportName" --output text 2>/dev/null || echo "")
    if [ -n "$CUR_REPORTS" ]; then
        for report in $CUR_REPORTS; do
            RESOURCES_TO_DELETE+=("  ✓ Legacy CUR Report: $report")
        done
    fi
    
    # Check for S3 buckets
    S3_BUCKETS=$(aws s3api list-buckets --query "Buckets[?contains(Name, 'finops')].Name" --output text 2>/dev/null || echo "")
    if [ -n "$S3_BUCKETS" ]; then
        for bucket in $S3_BUCKETS; do
            RESOURCES_TO_DELETE+=("  ✓ S3 Bucket (with ALL data): $bucket")
        done
    fi
    
    # Check for Glue databases
    if aws glue get-database --name cost_usage_db --region "$AWS_REGION" &>/dev/null; then
        RESOURCES_TO_DELETE+=("  ✓ Glue Database: cost_usage_db (with all tables)")
    fi
    
    # Check for Glue crawlers
    GLUE_CRAWLERS=$(aws glue list-crawlers --region "$AWS_REGION" --query "CrawlerNames[?contains(@, 'finops')]" --output text 2>/dev/null || echo "")
    if [ -n "$GLUE_CRAWLERS" ]; then
        for crawler in $GLUE_CRAWLERS; do
            RESOURCES_TO_DELETE+=("  ✓ Glue Crawler: $crawler")
        done
    fi
    
    # Check for Athena workgroups
    ATHENA_WORKGROUPS=$(aws athena list-work-groups --region "$AWS_REGION" --query "WorkGroups[?contains(Name, 'finops')].Name" --output text 2>/dev/null || echo "")
    if [ -n "$ATHENA_WORKGROUPS" ]; then
        for workgroup in $ATHENA_WORKGROUPS; do
            RESOURCES_TO_DELETE+=("  ✓ Athena Workgroup: $workgroup")
        done
    fi
    
    # Check for SSM Parameters
    SSM_PARAMETERS=$(aws ssm describe-parameters --region "$AWS_REGION" --parameter-filters "Key=Name,Option=Contains,Values=/${STACK_NAME}/" --query 'Parameters[].Name' --output text 2>/dev/null || echo "")
    if [ -n "$SSM_PARAMETERS" ]; then
        for param in $SSM_PARAMETERS; do
            RESOURCES_TO_DELETE+=("  ✓ SSM Parameter: $param")
        done
    fi
    
    # Check for Route53 DNS records (from deployment.env or state file)
    DNS_RECORDS_TO_DELETE=()
    ACM_CERTIFICATES_TO_DELETE=()
    if [ -f deployment.env ] && grep -q "DOMAIN_NAME" deployment.env; then
        DOMAIN_NAME=$(grep "DOMAIN_NAME" deployment.env | cut -d'=' -f2)
        HOSTED_ZONE_ID=$(grep "HOSTED_ZONE_ID" deployment.env | cut -d'=' -f2)
        if [ -n "$DOMAIN_NAME" ] && [ -n "$HOSTED_ZONE_ID" ]; then
            DNS_CHECK=$(aws route53 list-resource-record-sets --hosted-zone-id "$HOSTED_ZONE_ID" --query "ResourceRecordSets[?Name=='${DOMAIN_NAME}.']" --output text 2>/dev/null || echo "")
            if [ -n "$DNS_CHECK" ]; then
                RESOURCES_TO_DELETE+=("  ✓ Route53 DNS Record: $DOMAIN_NAME")
                DNS_RECORDS_TO_DELETE+=("$DOMAIN_NAME|$HOSTED_ZONE_ID")
            fi
            
            # Check for ACM certificates for this domain
            ACM_CERTS=$(aws acm list-certificates --region "$AWS_REGION" --query "CertificateSummaryList[?DomainName=='${DOMAIN_NAME}'].CertificateArn" --output text 2>/dev/null || echo "")
            if [ -n "$ACM_CERTS" ]; then
                for cert_arn in $ACM_CERTS; do
                    RESOURCES_TO_DELETE+=("  ✓ ACM Certificate: $DOMAIN_NAME")
                    ACM_CERTIFICATES_TO_DELETE+=("$cert_arn")
                done
            fi
        fi
    fi
    
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    echo "THE FOLLOWING RESOURCES WILL BE PERMANENTLY DELETED:"
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    
    if [ ${#RESOURCES_TO_DELETE[@]} -eq 0 ]; then
        log_info "No FinOps resources found to delete."
        exit 0
    fi
    
    for resource in "${RESOURCES_TO_DELETE[@]}"; do
        echo "$resource"
    done
    
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    echo ""
    log_error "⚠️  This action CANNOT be undone!"
    log_error "⚠️  All cost data, historical reports, and databases will be lost!"
    echo ""
    read -p "Type 'DELETE EVERYTHING' to confirm complete destruction: " CONFIRM
    
    if [ "$CONFIRM" != "DELETE EVERYTHING" ]; then
        log_info "Destruction cancelled."
        exit 0
    fi
    
    echo ""
    log_info "Starting complete destruction..."
    
    # Ensure required tools and credentials are available
    check_prerequisites
    
    # Load deployment metadata if present
    if [ -f deployment.env ]; then
        log_info "Loading deployment metadata from deployment.env"
        # shellcheck disable=SC1091
        source deployment.env || true
    fi
    
    # 1. Delete Data Exports (new API)
    if [ -n "$DATA_EXPORTS" ]; then
        for export in $DATA_EXPORTS; do
            log_info "Deleting Data Export: $export"
            EXPORT_ARN=$(aws bcm-data-exports list-exports --region us-east-1 --query "Exports[?Export.Name=='$export'].ExportArn" --output text)
            aws bcm-data-exports delete-export --export-arn "$EXPORT_ARN" --region us-east-1 2>/dev/null || log_warning "Failed to delete Data Export: $export"
        done
    fi
    
    # 2. Delete Legacy CUR reports
    if [ -n "$CUR_REPORTS" ]; then
        for report in $CUR_REPORTS; do
            log_info "Deleting Legacy CUR Report: $report"
            aws cur delete-report-definition --report-name "$report" --region us-east-1 2>/dev/null || log_warning "Failed to delete CUR report: $report"
        done
    fi
    
    # 3. Delete Glue database (this will delete all tables)
    log_info "Deleting Glue database: cost_usage_db"
    aws glue delete-database --name cost_usage_db --region "$AWS_REGION" 2>/dev/null || log_warning "Glue database not found or already deleted"
    
    # 4. Delete Glue crawlers
    if [ -n "$GLUE_CRAWLERS" ]; then
        for crawler in $GLUE_CRAWLERS; do
            log_info "Deleting Glue Crawler: $crawler"
            aws glue delete-crawler --name "$crawler" --region "$AWS_REGION" 2>/dev/null || log_warning "Failed to delete crawler: $crawler"
        done
    fi
    
    # 4.5. Delete IAM roles created by setup scripts
    log_info "Checking for FinOps IAM roles..."
    GLUE_ROLE_NAME="AWSGlueServiceRole-FinOps"
    if aws iam get-role --role-name "$GLUE_ROLE_NAME" &>/dev/null; then
        log_info "Deleting IAM role: $GLUE_ROLE_NAME"
        
        # Detach managed policies
        log_info "  - Detaching managed policies..."
        ATTACHED_POLICIES=$(aws iam list-attached-role-policies --role-name "$GLUE_ROLE_NAME" --query 'AttachedPolicies[].PolicyArn' --output text 2>/dev/null || echo "")
        for policy_arn in $ATTACHED_POLICIES; do
            log_info "    Detaching: $policy_arn"
            aws iam detach-role-policy --role-name "$GLUE_ROLE_NAME" --policy-arn "$policy_arn" 2>/dev/null || true
        done
        
        # Delete inline policies
        log_info "  - Deleting inline policies..."
        INLINE_POLICIES=$(aws iam list-role-policies --role-name "$GLUE_ROLE_NAME" --query 'PolicyNames' --output text 2>/dev/null || echo "")
        for policy_name in $INLINE_POLICIES; do
            log_info "    Deleting: $policy_name"
            aws iam delete-role-policy --role-name "$GLUE_ROLE_NAME" --policy-name "$policy_name" 2>/dev/null || true
        done
        
        # Delete the role
        log_info "  - Deleting role..."
        aws iam delete-role --role-name "$GLUE_ROLE_NAME" 2>/dev/null && \
            log_success "IAM role deleted: $GLUE_ROLE_NAME" || \
            log_warning "Failed to delete IAM role: $GLUE_ROLE_NAME"
    else
        log_info "IAM role not found: $GLUE_ROLE_NAME"
    fi
    
    # 4.6. Delete SSM Parameters (database password, etc.)
    if [ -n "$SSM_PARAMETERS" ]; then
        for param in $SSM_PARAMETERS; do
            log_info "Deleting SSM Parameter: $param"
            aws ssm delete-parameter --name "$param" --region "$AWS_REGION" 2>/dev/null || log_warning "Failed to delete parameter: $param"
        done
    else
        # Try to delete the database password parameter directly
        log_info "Deleting SSM Parameter: /${STACK_NAME}/database/password (if exists)"
        aws ssm delete-parameter --name "/${STACK_NAME}/database/password" --region "$AWS_REGION" 2>/dev/null || log_info "Parameter not found or already deleted"
    fi
    
    # 5. Delete CloudWatch Log Groups
    log_info "Checking for CloudWatch Log Groups..."
    LOG_GROUPS=$(aws logs describe-log-groups --region "$AWS_REGION" --log-group-name-prefix "/ecs/$STACK_NAME" --query 'logGroups[].logGroupName' --output text 2>/dev/null || echo "")
    if [ -n "$LOG_GROUPS" ]; then
        for log_group in $LOG_GROUPS; do
            log_info "Deleting CloudWatch Log Group: $log_group"
            aws logs delete-log-group --log-group-name "$log_group" --region "$AWS_REGION" 2>/dev/null || log_warning "Failed to delete log group: $log_group"
        done
    fi
    
    # 6. Delete Athena workgroups
    if [ -n "$ATHENA_WORKGROUPS" ]; then
        for workgroup in $ATHENA_WORKGROUPS; do
            log_info "Deleting Athena Workgroup: $workgroup"
            
            # Try to delete the workgroup with force option
            if aws athena delete-work-group \
                --work-group "$workgroup" \
                --recursive-delete-option \
                --region "$AWS_REGION" 2>&1; then
                log_success "  ✓ Workgroup deleted: $workgroup"
            else
                log_warning "  ✗ Failed to delete workgroup: $workgroup (may have running queries)"
                log_info "  Attempting force delete after brief wait..."
                sleep 5
                aws athena delete-work-group \
                    --work-group "$workgroup" \
                    --recursive-delete-option \
                    --region "$AWS_REGION" 2>/dev/null && \
                    log_success "  ✓ Workgroup deleted on retry" || \
                    log_warning "  ✗ Manual deletion may be required via AWS Console"
            fi
        done
    fi
    
    # 7. Empty and delete S3 buckets FIRST (before CloudFormation stack deletion)
    if [ -n "$S3_BUCKETS" ]; then
        for bucket in $S3_BUCKETS; do
            log_info "Emptying and deleting S3 bucket: $bucket"
            
            # Check if bucket exists
            if ! aws s3api head-bucket --bucket "$bucket" 2>/dev/null; then
                log_warning "Bucket $bucket does not exist or is not accessible, skipping"
                continue
            fi
            
            # Empty bucket first
            log_info "  - Removing all objects from bucket..."
            REMOVE_OUTPUT=$(aws s3 rm "s3://$bucket" --recursive 2>&1)
            if [ $? -ne 0 ]; then
                log_warning "Error removing objects: $REMOVE_OUTPUT"
            else
                log_success "  - Objects removed successfully"
            fi
            
            # Delete all versions (if versioning was enabled)
            log_info "  - Checking for object versions..."
            VERSIONS=$(aws s3api list-object-versions --bucket "$bucket" --query='{Objects: Versions[].{Key:Key,VersionId:VersionId}}' --max-items 1000 2>/dev/null || echo '{"Objects":[]}')
            VERSION_COUNT=$(echo "$VERSIONS" | jq -r '.Objects | length' 2>/dev/null || echo "0")
            
            if [ "$VERSION_COUNT" -gt 0 ]; then
                log_info "  - Removing $VERSION_COUNT object version(s)..."
                aws s3api delete-objects --bucket "$bucket" --delete "$VERSIONS" 2>/dev/null || log_warning "Failed to delete some versions"
            fi
            
            # Delete all delete markers
            log_info "  - Checking for delete markers..."
            DELETE_MARKERS=$(aws s3api list-object-versions --bucket "$bucket" --query='{Objects: DeleteMarkers[].{Key:Key,VersionId:VersionId}}' --max-items 1000 2>/dev/null || echo '{"Objects":[]}')
            MARKER_COUNT=$(echo "$DELETE_MARKERS" | jq -r '.Objects | length' 2>/dev/null || echo "0")
            
            if [ "$MARKER_COUNT" -gt 0 ]; then
                log_info "  - Removing $MARKER_COUNT delete marker(s)..."
                aws s3api delete-objects --bucket "$bucket" --delete "$DELETE_MARKERS" 2>/dev/null || log_warning "Failed to delete some markers"
            fi
            
            # Verify bucket is empty
            OBJECT_COUNT=$(aws s3 ls "s3://$bucket" --recursive 2>/dev/null | wc -l || echo "0")
            if [ "$OBJECT_COUNT" -gt 0 ]; then
                log_warning "  - Bucket still has $OBJECT_COUNT objects, attempting force delete"
            fi
            
            # Delete bucket now (CloudFormation won't try to delete it later)
            log_info "  - Deleting bucket..."
            DELETE_OUTPUT=$(aws s3api delete-bucket --bucket "$bucket" --region "$AWS_REGION" 2>&1)
            if [ $? -eq 0 ]; then
                log_success "  ✓ Bucket $bucket deleted successfully"
            else
                log_error "  ✗ Failed to delete bucket: $DELETE_OUTPUT"
                log_info "  - Try manually: aws s3 rb s3://$bucket --force"
            fi
        done
    fi
    
    # 7. Delete CloudFormation stacks and ECR (using existing function)
    log_info "Deleting CloudFormation stacks and ECR repositories..."
    remove_rds_delete_protection
    
    aws cloudformation delete-stack --stack-name "${STACK_NAME}-services" --region "$AWS_REGION" 2>/dev/null || true
    log_info "Waiting for services stack deletion..."
    aws cloudformation wait stack-delete-complete --stack-name "${STACK_NAME}-services" --region "$AWS_REGION" 2>/dev/null || true
    
    aws cloudformation delete-stack --stack-name "$STACK_NAME" --region "$AWS_REGION" 2>/dev/null || true
    log_info "Waiting for main stack deletion..."
    aws cloudformation wait stack-delete-complete --stack-name "$STACK_NAME" --region "$AWS_REGION" 2>/dev/null || true
    
    aws ecr delete-repository --repository-name finops-backend --force --region "$AWS_REGION" 2>/dev/null || true
    aws ecr delete-repository --repository-name finops-frontend --force --region "$AWS_REGION" 2>/dev/null || true
    
    # 7.5. Comprehensive VPC and Network Cleanup
    log_info "Performing comprehensive network resource cleanup..."
    
    # Find all FinOps-related VPCs
    FINOPS_VPCS=$(aws ec2 describe-vpcs --region "$AWS_REGION" \
        --filters "Name=tag:Name,Values=*${STACK_NAME}*" \
        --query 'Vpcs[].VpcId' --output text 2>/dev/null || echo "")
    
    if [ -n "$FINOPS_VPCS" ]; then
        log_info "Found VPCs to clean up: $FINOPS_VPCS"
        
        for vpc in $FINOPS_VPCS; do
            log_info "  Cleaning up VPC: $vpc"
            
            # Delete NAT Gateways first (they take time to delete)
            NAT_GWS=$(aws ec2 describe-nat-gateways --region "$AWS_REGION" \
                --filter "Name=vpc-id,Values=$vpc" "Name=state,Values=available,pending,deleting" \
                --query 'NatGateways[].NatGatewayId' --output text 2>/dev/null || echo "")
            
            if [ -n "$NAT_GWS" ]; then
                for nat in $NAT_GWS; do
                    log_info "    Deleting NAT Gateway: $nat"
                    aws ec2 delete-nat-gateway --nat-gateway-id "$nat" --region "$AWS_REGION" 2>/dev/null || true
                done
                log_info "    Waiting 30s for NAT Gateways to start deletion..."
                sleep 30
            fi
            
            # Delete Load Balancers
            LBS=$(aws elbv2 describe-load-balancers --region "$AWS_REGION" \
                --query "LoadBalancers[?VpcId=='$vpc'].LoadBalancerArn" --output text 2>/dev/null || echo "")
            
            if [ -n "$LBS" ]; then
                for lb in $LBS; do
                    log_info "    Deleting Load Balancer: $lb"
                    aws elbv2 delete-load-balancer --load-balancer-arn "$lb" --region "$AWS_REGION" 2>/dev/null || true
                done
                log_info "    Waiting 30s for Load Balancers to delete..."
                sleep 30
            fi
            
            # Delete Network Interfaces (ENIs)
            log_info "    Checking for Network Interfaces..."
            ENIS=$(aws ec2 describe-network-interfaces --region "$AWS_REGION" \
                --filters "Name=vpc-id,Values=$vpc" \
                --query 'NetworkInterfaces[].NetworkInterfaceId' --output text 2>/dev/null || echo "")
            
            if [ -n "$ENIS" ]; then
                for eni in $ENIS; do
                    ENI_STATUS=$(aws ec2 describe-network-interfaces --region "$AWS_REGION" \
                        --network-interface-ids "$eni" \
                        --query 'NetworkInterfaces[0].Status' --output text 2>/dev/null || echo "unknown")
                    
                    if [ "$ENI_STATUS" = "available" ]; then
                        log_info "    Deleting ENI: $eni"
                        aws ec2 delete-network-interface --network-interface-id "$eni" --region "$AWS_REGION" 2>/dev/null || \
                            log_warning "    Failed to delete ENI: $eni"
                    else
                        log_warning "    ENI $eni is in state: $ENI_STATUS (skipping)"
                    fi
                done
            fi
            
            # Remove all ingress rules from security groups to break dependencies
            log_info "    Removing security group dependencies..."
            SG_IDS=$(aws ec2 describe-security-groups --region "$AWS_REGION" \
                --filters "Name=vpc-id,Values=$vpc" \
                --query 'SecurityGroups[?GroupName!=`default`].GroupId' --output text 2>/dev/null || echo "")
            
            if [ -n "$SG_IDS" ]; then
                for sg in $SG_IDS; do
                    # Revoke all ingress rules
                    INGRESS_RULES=$(aws ec2 describe-security-group-rules --region "$AWS_REGION" \
                        --filters "Name=group-id,Values=$sg" \
                        --query 'SecurityGroupRules[?IsEgress==`false`].SecurityGroupRuleId' \
                        --output text 2>/dev/null || echo "")
                    
                    if [ -n "$INGRESS_RULES" ]; then
                        aws ec2 revoke-security-group-ingress --group-id "$sg" \
                            --security-group-rule-ids $INGRESS_RULES \
                            --region "$AWS_REGION" 2>/dev/null || true
                    fi
                done
                
                # Now delete the security groups
                log_info "    Deleting security groups..."
                for sg in $SG_IDS; do
                    SG_NAME=$(aws ec2 describe-security-groups --region "$AWS_REGION" \
                        --group-ids "$sg" --query 'SecurityGroups[0].GroupName' --output text 2>/dev/null || echo "Unknown")
                    
                    aws ec2 delete-security-group --group-id "$sg" --region "$AWS_REGION" 2>/dev/null && \
                        log_success "    ✓ Deleted security group: $sg ($SG_NAME)" || \
                        log_warning "    ✗ Failed to delete security group: $sg ($SG_NAME)"
                done
            fi
            
            # Delete Subnets
            log_info "    Deleting subnets..."
            SUBNETS=$(aws ec2 describe-subnets --region "$AWS_REGION" \
                --filters "Name=vpc-id,Values=$vpc" \
                --query 'Subnets[].SubnetId' --output text 2>/dev/null || echo "")
            
            if [ -n "$SUBNETS" ]; then
                for subnet in $SUBNETS; do
                    aws ec2 delete-subnet --subnet-id "$subnet" --region "$AWS_REGION" 2>/dev/null || \
                        log_warning "    Failed to delete subnet: $subnet"
                done
            fi
            
            # Detach and delete Internet Gateways
            log_info "    Deleting Internet Gateways..."
            IGWS=$(aws ec2 describe-internet-gateways --region "$AWS_REGION" \
                --filters "Name=attachment.vpc-id,Values=$vpc" \
                --query 'InternetGateways[].InternetGatewayId' --output text 2>/dev/null || echo "")
            
            if [ -n "$IGWS" ]; then
                for igw in $IGWS; do
                    aws ec2 detach-internet-gateway --internet-gateway-id "$igw" --vpc-id "$vpc" --region "$AWS_REGION" 2>/dev/null || true
                    aws ec2 delete-internet-gateway --internet-gateway-id "$igw" --region "$AWS_REGION" 2>/dev/null || \
                        log_warning "    Failed to delete IGW: $igw"
                done
            fi
            
            # Delete Route Tables (non-main)
            log_info "    Deleting route tables..."
            ROUTE_TABLES=$(aws ec2 describe-route-tables --region "$AWS_REGION" \
                --filters "Name=vpc-id,Values=$vpc" \
                --query 'RouteTables[?Associations[0].Main!=`true`].RouteTableId' --output text 2>/dev/null || echo "")
            
            if [ -n "$ROUTE_TABLES" ]; then
                for rt in $ROUTE_TABLES; do
                    aws ec2 delete-route-table --route-table-id "$rt" --region "$AWS_REGION" 2>/dev/null || \
                        log_warning "    Failed to delete route table: $rt"
                done
            fi
            
            # Delete VPC Endpoints
            log_info "    Deleting VPC endpoints..."
            VPC_ENDPOINTS=$(aws ec2 describe-vpc-endpoints --region "$AWS_REGION" \
                --filters "Name=vpc-id,Values=$vpc" \
                --query 'VpcEndpoints[].VpcEndpointId' --output text 2>/dev/null || echo "")
            
            if [ -n "$VPC_ENDPOINTS" ]; then
                for endpoint in $VPC_ENDPOINTS; do
                    aws ec2 delete-vpc-endpoints --vpc-endpoint-ids "$endpoint" --region "$AWS_REGION" 2>/dev/null || \
                        log_warning "    Failed to delete VPC endpoint: $endpoint"
                done
            fi
            
            # Finally, try to delete the VPC
            log_info "    Attempting to delete VPC: $vpc"
            if aws ec2 delete-vpc --vpc-id "$vpc" --region "$AWS_REGION" 2>/dev/null; then
                log_success "    ✓ VPC deleted successfully: $vpc"
            else
                log_error "    ✗ Failed to delete VPC: $vpc"
                log_warning "    VPC may still have dependencies. Manual cleanup may be required."
            fi
        done
    fi
    
    # 7.6. Release orphaned Elastic IPs
    log_info "Checking for orphaned Elastic IPs..."
    ORPHANED_EIPS=$(aws ec2 describe-addresses --region "$AWS_REGION" \
        --filters "Name=domain,Values=vpc" \
        --query 'Addresses[?AssociationId==`null`].AllocationId' --output text 2>/dev/null || echo "")
    
    if [ -n "$ORPHANED_EIPS" ]; then
        for eip in $ORPHANED_EIPS; do
            EIP_ADDRESS=$(aws ec2 describe-addresses --region "$AWS_REGION" \
                --allocation-ids "$eip" --query 'Addresses[0].PublicIp' --output text 2>/dev/null || echo "Unknown")
            log_info "Releasing Elastic IP: $eip ($EIP_ADDRESS)"
            aws ec2 release-address --allocation-id "$eip" --region "$AWS_REGION" 2>/dev/null || \
                log_warning "Failed to release Elastic IP: $eip"
        done
    fi
    
    # 7.7. Delete Route53 DNS records
    if [ ${#DNS_RECORDS_TO_DELETE[@]} -gt 0 ]; then
        for dns_entry in "${DNS_RECORDS_TO_DELETE[@]}"; do
            DOMAIN_NAME=$(echo "$dns_entry" | cut -d'|' -f1)
            HOSTED_ZONE_ID=$(echo "$dns_entry" | cut -d'|' -f2)
            
            log_info "Deleting Route53 DNS record: $DOMAIN_NAME"
            
            # Get the full record set details
            RECORD_JSON=$(aws route53 list-resource-record-sets \
                --hosted-zone-id "$HOSTED_ZONE_ID" \
                --query "ResourceRecordSets[?Name=='${DOMAIN_NAME}.']" \
                --output json 2>/dev/null)
            
            if [ -n "$RECORD_JSON" ] && [ "$RECORD_JSON" != "[]" ]; then
                # Extract the first record (should only be one)
                RECORD=$(echo "$RECORD_JSON" | jq '.[0]' 2>/dev/null)
                
                if [ -n "$RECORD" ] && [ "$RECORD" != "null" ]; then
                    # Create change batch for deletion
                    CHANGE_BATCH="{\"Changes\":[{\"Action\":\"DELETE\",\"ResourceRecordSet\":$RECORD}]}"
                    
                    aws route53 change-resource-record-sets \
                        --hosted-zone-id "$HOSTED_ZONE_ID" \
                        --change-batch "$CHANGE_BATCH" 2>/dev/null && \
                        log_success "DNS record deleted: $DOMAIN_NAME" || \
                        log_warning "Failed to delete DNS record: $DOMAIN_NAME"
                else
                    log_warning "Could not parse DNS record for $DOMAIN_NAME"
                fi
            else
                log_info "DNS record $DOMAIN_NAME not found or already deleted"
            fi
        done
    fi
    
    # 7.6. Delete ACM certificates
    if [ ${#ACM_CERTIFICATES_TO_DELETE[@]} -gt 0 ]; then
        for cert_arn in "${ACM_CERTIFICATES_TO_DELETE[@]}"; do
            log_info "Deleting ACM certificate: $cert_arn"
            
            # Check certificate status first
            CERT_STATUS=$(aws acm describe-certificate --certificate-arn "$cert_arn" --region "$AWS_REGION" --query 'Certificate.Status' --output text 2>/dev/null || echo "NOT_FOUND")
            
            if [ "$CERT_STATUS" = "NOT_FOUND" ]; then
                log_info "Certificate already deleted or not found"
            elif [ "$CERT_STATUS" = "ISSUED" ] || [ "$CERT_STATUS" = "PENDING_VALIDATION" ] || [ "$CERT_STATUS" = "VALIDATION_TIMED_OUT" ]; then
                aws acm delete-certificate --certificate-arn "$cert_arn" --region "$AWS_REGION" 2>/dev/null && \
                    log_success "ACM certificate deleted" || \
                    log_warning "Failed to delete ACM certificate (may be in use)"
            else
                log_warning "Certificate in unexpected status: $CERT_STATUS"
            fi
        done
    fi
    
    # 8. Clean up state files
    rm -f "$ENV_FILE"
    rm -f deployment.env.backup
    
    # Print summary
    print_destroyAll_summary
}

# Clean up function
cleanup() {
    log_info "Cleaning up deployment artifacts..."
    # Don't remove deployment.env - it's needed for subsequent deployments
    # Only remove temporary files
    rm -f .temp_migration_output 2>/dev/null || true
}

# Main deployment process
main() {
    log_info "Starting FinOps AI Cost Intelligence Platform deployment..."
    
    check_prerequisites
    detect_existing_infrastructure
    deploy_infrastructure
    validate_athena_data
    log_info "Athena validation finished. Proceeding to image build."
    build_and_push_images
    log_info "Docker image build step finished. Deploying ECS services stack next."
    deploy_services

    # Run DB migrations right after services deploy
    log_info "Preparing to run database migrations..."
    
    # Give ECS services time to stabilize
    log_info "Waiting for ECS tasks to reach running state (30 seconds)..."
    sleep 30
    
    # Run migrations with validation
    if run_migrations_ecs; then
        log_success "Database migrations completed successfully"
        
        # Validate migration state
        log_info "Validating database schema..."
        if validate_migration_state; then
            log_success "Database schema validation passed"
        else
            log_warning "Database schema validation could not be completed"
            log_info "Manual verification recommended: scripts/deployment/aws_run_migrations.sh"
        fi
    else
        log_error "Database migrations failed"
        log_warning "Deployment completed but database schema may be incomplete"
        log_info "To retry migrations: scripts/deployment/aws_run_migrations.sh exec --region $AWS_REGION --cluster <cluster-name> --service <service-name> --container backend"
        log_info "Or manually via ECS console: Run command 'alembic upgrade head' in backend container"
        
        read -p "Continue despite migration failure? (yes/no): " continue_choice
        if [ "$continue_choice" != "yes" ]; then
            log_error "Deployment halted due to migration failure"
            exit 1
        fi
    fi
    
    # Validate deployment before showing summary
    validate_deployment
    
    get_deployment_info
    
    # Print deployment summary
    print_deployment_summary
}

# Handle script termination
trap cleanup EXIT

# Parse command line arguments
# Parse flags (supports placing flags before or after main command)
COMMAND="deploy"
for arg in "$@"; do
    case "$arg" in
        deploy|destroy|destroyAll|update)
            COMMAND="$arg" ;;
        --help|-h)
            echo "Usage: $0 [command]"
            echo "Commands:"
            echo "  deploy        Deploy complete infrastructure and services"
            echo "  destroy       Destroy CloudFormation stacks and ECR (keeps CUR, S3, Glue, Athena)"
            echo "  destroyAll    Destroy EVERYTHING including CUR/Data Exports, S3 buckets, Glue, Athena"
            echo "  update        Update services with new Docker images"
            exit 0 ;;
    esac
done

case "$COMMAND" in
    deploy)
        main ;;
    destroy)
        destroy_infrastructure ;;
    destroyAll)
        destroy_all ;;
    update)
        log_info "Updating services..."
        build_and_push_images
        deploy_services
        get_deployment_info ;;
    *)
        echo "Unknown command: $COMMAND" >&2
        exit 1 ;;
esac

